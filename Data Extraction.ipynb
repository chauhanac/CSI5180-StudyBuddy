{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4f6c135",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import nltk\n",
    "import string\n",
    "import PyPDF2\n",
    "import requests\n",
    "import builtins\n",
    "import warnings\n",
    "import wikipedia\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import language_tool_python\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a0091c",
   "metadata": {},
   "source": [
    "### The following are helper methods used for Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf1f06a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_tool = language_tool_python.LanguageTool('en-US')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05c8c13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_utf8_chars(text):\n",
    "    pattern = r'[^\\x00-\\x7F]+'  # Pattern to match non-UTF-8 characters\n",
    "    return re.sub(pattern, ' ', text)\n",
    "\n",
    "def grammar_check(text):\n",
    "    matches = lang_tool.check(text)\n",
    "    return len(matches)\n",
    "\n",
    "def remove_encoded_chars(text):\n",
    "    clean_text = \"\"\n",
    "    for c in text:\n",
    "        if not unicodedata.category(c) == 'Co':\n",
    "            clean_text+=c\n",
    "    return clean_text\n",
    "\n",
    "def check_and_remove(text, tags):\n",
    "    for tag in tags:\n",
    "        if tag in text:\n",
    "            text = text.replace(tag, \" \")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "842e27df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "\n",
    "    tags_removed = ['caroline barriÃ¨re', 'csi5180', 'winter 2024', 'csi4106' , 'fall 2023']\n",
    "    \n",
    "    translator = str.maketrans(\" \", \" \", string.punctuation)\n",
    "    text = text.translate(translator)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip() # Remove extra spaces\n",
    "\n",
    "    text = text.lower().strip()\n",
    "    words = word_tokenize(text) \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    text = \" \".join(word for word in words)\n",
    "    \n",
    "    text = check_and_remove(text, tags_removed) # REMOVES COMMON OCCURRING TAGS\n",
    "    text = remove_non_utf8_chars(text) # REMOVES NON UTF-8 CHRACTERS\n",
    "    text = remove_encoded_chars(text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "560684c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, chunk_size=300):\n",
    "\n",
    "    words = text.split()  # Split the text into a list of words\n",
    "    chunk = []\n",
    "\n",
    "    for word in words:\n",
    "        chunk.append(word)\n",
    "        if len(chunk) >= chunk_size:\n",
    "            yield ' '.join(chunk)  # Yield a chunk as a joined string\n",
    "            chunk = []  # Reset the chunk\n",
    "\n",
    "    # Yield the final chunk (if any words are left)\n",
    "    if chunk:\n",
    "        yield ' '.join(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a278a5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = './'\n",
    "pdf_files = [f for f in os.listdir(folder_path) if f.endswith('.pdf')]\n",
    "text_results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd79aa83",
   "metadata": {},
   "source": [
    "### Extract textual data from the course PDFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6609c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pdf_text(pdf_file):\n",
    "    \n",
    "    with open(pdf_file, 'rb') as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "\n",
    "        # Extract Text (page-by-page)\n",
    "        text_data = []\n",
    "        num_pages = len(reader.pages)\n",
    "        for page_num in range(num_pages):\n",
    "            page = reader.pages[page_num]\n",
    "            page_text = page.extract_text()\n",
    "            page_text = clean_text(page_text)\n",
    "            \n",
    "            if len(page_text.split()) > 20:\n",
    "                text_data.append(page_text)\n",
    "            else:\n",
    "                text_data.append(\"\")\n",
    "\n",
    "        return text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf51a77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_name in pdf_files:\n",
    "    \n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    try:\n",
    "        text_data = extract_pdf_text(file_path)\n",
    "        combined_text = \" \".join(text_data)\n",
    "        text_results[file_name] = combined_text\n",
    "        \n",
    "    except PyPDF2.errors.PdfReadError:\n",
    "        print(f\"Error processing file (pdferror): {file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f0d7fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_text_dataframe = pd.DataFrame(columns=['text', 'text_length','gram_score'])\n",
    "\n",
    "\n",
    "for texts in text_results.values():\n",
    "    chunks = chunk_text(texts)\n",
    "    \n",
    "    for t in chunks:\n",
    "        row = [t, len(t.split()) ,grammar_check(t)]\n",
    "        pdf_text_dataframe.loc[len(pdf_text_dataframe)] = row\n",
    "        \n",
    "\n",
    "pdf_text_dataframe.to_csv(\"PDF_file_text_300.csv\", index= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecf9cf2",
   "metadata": {},
   "source": [
    "### Extract textual data from URLs referenced in the course PDFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "190b8026",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_urls = [] # Stores the URLs that point to a PDF file\n",
    "forbidden_urls = [] # Keeps track of the URLs that could not be accessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8c87fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns a list of all the unique URLs\n",
    "def extract_urls(pdf_file):\n",
    "    \n",
    "    urls = []\n",
    "    with open(pdf_file, 'rb') as file:\n",
    "        \n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        num_pages = len(reader.pages)\n",
    "\n",
    "        for page_num in range(num_pages):\n",
    "            page = reader.pages[page_num]\n",
    "            objects = page.get('/Annots', {})\n",
    "            if not objects:\n",
    "                continue\n",
    "                \n",
    "            for obj in objects:\n",
    "                annot = obj.get_object()\n",
    "                \n",
    "                if annot['/Subtype'] == '/Link' and '/A' in annot:\n",
    "                    if '/URI' in annot['/A']:\n",
    "                        url = annot['/A'].get_object()['/URI']\n",
    "                        urls.append(url)\n",
    "                    elif '/S' in annot['/A'] and annot['/A']['/S'] == '/URI':    \n",
    "                        url = annot['/A']['/URI']\n",
    "                        urls.append(url)\n",
    "                \n",
    "    urls = list(set(urls)) # Avoids repetation of URLs\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9bc22d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets texts from webpages and cleans it for processing and updates the the PDF URLs.\n",
    "def extract_text_from_url(url):\n",
    "    \n",
    "    if is_pdf_url_headers(url):\n",
    "        return \"\"\n",
    "    else:\n",
    "        text = extract_text_from_webpage(url)\n",
    "\n",
    "    lines = (line.strip() for line in text.splitlines())\n",
    "    chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "    text = \" \".join(chunk for chunk in chunks if chunk)\n",
    "    \n",
    "    text = clean_text(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a64b06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts and returns text from a webpage\n",
    "def extract_text_from_webpage(url):\n",
    "    \n",
    "    extracted_text = \" \"\n",
    "    try:\n",
    "        response = requests.get(url, timeout=5)\n",
    "        response.raise_for_status()  # Raise an exception for error status codes\n",
    "\n",
    "        soup = BeautifulSoup(response.content, 'html.parser', from_encoding=\"iso-8859-1\")\n",
    "        for element in soup([\"script\", \"style\"]):\n",
    "            element.extract() # Remove script and style elements as they often contain non-content text\n",
    "\n",
    "        content_elements = soup.find_all(['p']) \n",
    "        for element in content_elements:\n",
    "            extracted_text += element.get_text()\n",
    "        \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching URL: {e}\")\n",
    "        forbidden_urls.append(url)\n",
    "        extracted_text+= \" \"\n",
    "\n",
    "    except requests.exceptions.Timeout as e:\n",
    "        print(f\"Error fetching URL: {e}\")\n",
    "        forbidden_urls.append(url)\n",
    "        extracted_text+= \" \"\n",
    "        \n",
    "    return extracted_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "faff1069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checks if the given URL points to a PDF or not\n",
    "def is_pdf_url_headers(pdf_url):\n",
    "    try:\n",
    "        response = requests.head(pdf_url, timeout=5)\n",
    "        content_type = response.headers.get('content-type')\n",
    "        return content_type == 'application/pdf'\n",
    "    \n",
    "    except requests.exceptions.RequestException:\n",
    "        return False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6ce88070",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|âââââââââââââââââââââââââââââââââââââââââââ| 17/17 [00:00<00:00, 72.99it/s]\n"
     ]
    }
   ],
   "source": [
    "# GET LIST OF ALL URLs\n",
    "url_list = []\n",
    "\n",
    "for file_name in tqdm(pdf_files):\n",
    "\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    try:\n",
    "        urls = extract_urls(file_path)\n",
    "        for i in urls:\n",
    "            url_list.append(i)\n",
    "    except PyPDF2.errors.PdfReadError:\n",
    "        print(f\"Error processing file (pdferror): {file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c85ee4be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|â                                          | 2/249 [00:04<07:55,  1.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching URL: 403 Client Error: Forbidden for url: https://www.datacamp.com/tutorial\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|ââ                                         | 9/249 [00:23<07:05,  1.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching URL: 403 Client Error: Forbidden for url: https://www.researchgate.net/figure/Illustration-of-the-BERT-model-for-joint-intent-detection-and-slot-filling_fig3_359661835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  4%|ââ                                        | 10/249 [00:24<05:28,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching URL: 403 Client Error: Forbidden for url: https://www.researchgate.net/figure/Basic-models-for-fingerprint-verification-and-identification-processes_fig1_215585741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  4%|ââ                                        | 11/249 [00:24<04:17,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching URL: 403 Client Error: Forbidden for url: https://www.researchgate.net/figure/The-general-pipeline-for-face-verification-in-this-paper-where-classifier-loss-function_fig6_323025952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|ââââ                                      | 22/249 [00:49<08:33,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching URL: 403 Client Error: Forbidden for url: https://learnopencv.com/face-recognition-an-introduction-for-beginners/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  9%|ââââ                                      | 23/249 [00:49<06:28,  1.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching URL: 403 Client Error: Forbidden for url: https://www.researchgate.net/figure/Basic-models-for-fingerprint-verification-and-identification-processes_fig1_215585741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|âââââ                                     | 27/249 [00:57<06:25,  1.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching URL: 403 Client Error: Forbidden for url: https://www.researchgate.net/figure/The-general-pipeline-for-face-verification-in-this-paper-where-classifier-loss-function_fig6_323025952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|âââââ                                     | 29/249 [01:04<08:18,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching URL: 403 Client Error: Forbidden for url: https://www.researchgate.net/figure/Singular-points-and-minutiae-types-in-a-fingerprint_fig5_306328339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|âââââ                                     | 30/249 [01:05<06:49,  1.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching URL: 403 Client Error: Forbidden for url: https://www.iqt.org/voices-at-speech-odyssey-2020-advances-in-speaker-embeddings/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|ââââââ                                    | 32/249 [01:21<20:46,  5.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching URL: HTTPSConnectionPool(host='www.embedded.com', port=443): Read timed out. (read timeout=5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|âââââââ                                   | 37/249 [01:36<10:11,  2.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching URL: 403 Client Error: Forbidden for url: https://voicebot.ai/2018/03/21/data-breakdown-consumers-use-smart-speakers-today/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|âââââââââ                                 | 52/249 [02:38<19:13,  5.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching URL: 410 Client Error: Gone for url: https://medium.com/ibm-data-ai/ibm-watson-text-to-speech-neural-voices-added-to-service-e562106ff9c7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|ââââââââââ                                | 56/249 [02:52<11:40,  3.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching URL: 403 Client Error: Forbidden for url: https://www.explainthatstuff.com/how-speech-synthesis-works.html\n",
      "Error fetching URL: HTTPSConnectionPool(host='text-to-speech-demo.ng.bluemix.net', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x16adc2e30>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|ââââââââââ                                | 60/249 [02:59<07:36,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching URL: 403 Client Error: Forbidden for url: https://venturebeat.com/2019/04/16/how-to-prevent-alexa-cortana-siri-google-assistant-and-bixby-from-recording-you/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|ââââââââââââ                              | 68/249 [03:31<11:21,  3.77s/it]Certificate did not match expected hostname: lionbridge.ai. Certificate: {'subject': ((('countryName', 'US'),), (('stateOrProvinceName', 'Massachusetts'),), (('localityName', 'Waltham'),), (('organizationName', 'Lionbridge Technologies, LLC'),), (('commonName', '*.lionbridge.com'),)), 'issuer': ((('countryName', 'US'),), (('organizationName', 'DigiCert Inc'),), (('commonName', 'DigiCert Global G2 TLS RSA SHA256 2020 CA1'),)), 'version': 3, 'serialNumber': '0C6F36D65B0D8DA95AF5457D07A78D06', 'notBefore': 'Oct 10 00:00:00 2023 GMT', 'notAfter': 'Oct 23 23:59:59 2024 GMT', 'subjectAltName': (('DNS', '*.lionbridge.com'), ('DNS', 'lionbridge.com')), 'OCSP': ('http://ocsp.digicert.com',), 'caIssuers': ('http://cacerts.digicert.com/DigiCertGlobalG2TLSRSASHA2562020CA1-1.crt',), 'crlDistributionPoints': ('http://crl3.digicert.com/DigiCertGlobalG2TLSRSASHA2562020CA1-1.crl', 'http://crl4.digicert.com/DigiCertGlobalG2TLSRSASHA2562020CA1-1.crl')}\n",
      "Certificate did not match expected hostname: lionbridge.ai. Certificate: {'subject': ((('countryName', 'US'),), (('stateOrProvinceName', 'Massachusetts'),), (('localityName', 'Waltham'),), (('organizationName', 'Lionbridge Technologies, LLC'),), (('commonName', '*.lionbridge.com'),)), 'issuer': ((('countryName', 'US'),), (('organizationName', 'DigiCert Inc'),), (('commonName', 'DigiCert Global G2 TLS RSA SHA256 2020 CA1'),)), 'version': 3, 'serialNumber': '0C6F36D65B0D8DA95AF5457D07A78D06', 'notBefore': 'Oct 10 00:00:00 2023 GMT', 'notAfter': 'Oct 23 23:59:59 2024 GMT', 'subjectAltName': (('DNS', '*.lionbridge.com'), ('DNS', 'lionbridge.com')), 'OCSP': ('http://ocsp.digicert.com',), 'caIssuers': ('http://cacerts.digicert.com/DigiCertGlobalG2TLSRSASHA2562020CA1-1.crt',), 'crlDistributionPoints': ('http://crl3.digicert.com/DigiCertGlobalG2TLSRSASHA2562020CA1-1.crl', 'http://crl4.digicert.com/DigiCertGlobalG2TLSRSASHA2562020CA1-1.crl')}\n",
      "Certificate did not match expected hostname: lionbridge.ai. Certificate: {'subject': ((('countryName', 'US'),), (('stateOrProvinceName', 'Massachusetts'),), (('localityName', 'Waltham'),), (('organizationName', 'Lionbridge Technologies, LLC'),), (('commonName', '*.lionbridge.com'),)), 'issuer': ((('countryName', 'US'),), (('organizationName', 'DigiCert Inc'),), (('commonName', 'DigiCert Global G2 TLS RSA SHA256 2020 CA1'),)), 'version': 3, 'serialNumber': '0C6F36D65B0D8DA95AF5457D07A78D06', 'notBefore': 'Oct 10 00:00:00 2023 GMT', 'notAfter': 'Oct 23 23:59:59 2024 GMT', 'subjectAltName': (('DNS', '*.lionbridge.com'), ('DNS', 'lionbridge.com')), 'OCSP': ('http://ocsp.digicert.com',), 'caIssuers': ('http://cacerts.digicert.com/DigiCertGlobalG2TLSRSASHA2562020CA1-1.crt',), 'crlDistributionPoints': ('http://crl3.digicert.com/DigiCertGlobalG2TLSRSASHA2562020CA1-1.crl', 'http://crl4.digicert.com/DigiCertGlobalG2TLSRSASHA2562020CA1-1.crl')}\n",
      " 28%|ââââââââââââ                              | 69/249 [03:32<08:23,  2.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching URL: HTTPSConnectionPool(host='lionbridge.ai', port=443): Max retries exceeded with url: /datasets/best-speech-recognition-datasets-for-machine-learning/ (Caused by SSLError(CertificateError(\"hostname 'lionbridge.ai' doesn't match either of '*.lionbridge.com', 'lionbridge.com'\")))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|âââââââââââââââââ                        | 101/249 [05:12<08:19,  3.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching URL: 404 Client Error: Not Found for url: https://www.kormax.co.kr/en/sound-and-ultrasound/mechanism-for-generating-the-human-voice/?ckattempt=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|âââââââââââââââââââ                      | 114/249 [06:24<12:56,  5.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching URL: HTTPSConnectionPool(host='text-to-speech-demo.ng.bluemix.net', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x16ac43010>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|ââââââââââââââââââââ                     | 119/249 [06:44<10:12,  4.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching URL: 403 Client Error: Forbidden for url: https://www.voices.com/blog/accessible-technology/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 48%|ââââââââââââââââââââ                     | 120/249 [06:45<07:33,  3.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching URL: 403 Client Error: Forbidden for url: https://venturebeat.com/2019/05/21/amazons-ai-improves-emotion-detection-in-voices/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|âââââââââââââââââââââ                    | 123/249 [06:53<06:14,  2.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching URL: 403 Client Error: Forbidden for url: https://www.researchgate.net/publication/335948444_Speech_Synthesis_Evaluation_-_State-of-the-Art_Assessment_and_Suggestion_for_a_Novel_Research_Program\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|ââââââââââââââââââââââ                   | 130/249 [07:13<04:44,  2.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching URL: 404 Client Error: Not Found for url: https://opendatahub.io/news/2019-09-04/sentiment-analysis-blog.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 53%|ââââââââââââââââââââââ                   | 131/249 [07:13<03:33,  1.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching URL: 403 Client Error: Forbidden for url: https://www.voices.com/blog/text-to-speech-software-use-cases/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|âââââââââââââââââââââââââ                | 147/249 [08:25<05:23,  3.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching URL: 403 Client Error: Forbidden for url: https://www.explainthatstuff.com/how-speech-synthesis-works.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 59%|âââââââââââââââââââââââââ                | 148/249 [08:25<03:58,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching URL: 403 Client Error: Forbidden for url: https://www.researchgate.net/figure/An-illustration-of-the-phoneme-mapping-procedure-used-for-DNN-training-a-The-speech_fig4_341084647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|âââââââââââââââââââââââââ                | 149/249 [08:26<02:57,  1.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching URL: 403 Client Error: Forbidden for url: https://www.researchgate.net/figure/Workflow-of-BERT-Sentiment-Analysis-detailing-Self-Attention_fig3_352081817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|ââââââââââââââââââââââââââ               | 156/249 [09:11<05:44,  3.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching URL: 403 Client Error: Forbidden for url: https://www.researchgate.net/figure/Concatenative-synthesis-of-an-activation-command-The-MFCC-feature-for-each-segment-in-a_fig5_319415640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|ââââââââââââââââââââââââââ               | 158/249 [09:27<09:58,  6.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching URL: HTTPSConnectionPool(host='wiki.inf.ed.ac.uk', port=443): Max retries exceeded with url: /twiki/pub/CSTR/Speak14To15/evaluation.pdf (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x16abaa9b0>, 'Connection to wiki.inf.ed.ac.uk timed out. (connect timeout=5)'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|âââââââââââââââââââââââââââ              | 162/249 [09:46<07:57,  5.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching URL: 410 Client Error: Gone for url: https://medium.com/ibm-data-ai/ibm-watson-text-to-speech-neural-voices-added-to-service-e562106ff9c7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|âââââââââââââââââââââââââââ              | 164/249 [09:51<05:20,  3.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching URL: 403 Client Error: Forbidden for url: https://www.researchgate.net/figure/Waveforms-and-pitch-contours-for-examples-of-the-happy-left-and-sad-right-pitch_fig3_281403058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|ââââââââââââââââââââââââââââ             | 169/249 [10:11<05:00,  3.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching URL: 403 Client Error: Forbidden for url: https://voicebot.ai/2018/05/14/google-home-beats-amazon-echo-in-two-audio-recognition-performance-tests-but-alexa-delivers-highest-composite-score/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|ââââââââââââââââââââââââââââââ           | 182/249 [10:49<02:22,  2.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching URL: 406 Client Error: Not Acceptable for url: https://www.aclweb.org/anthology/W18-6529.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|âââââââââââââââââââââââââââââââ          | 184/249 [10:53<02:13,  2.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching URL: 406 Client Error: Not Acceptable for url: https://www.aclweb.org/anthology/venues/inlg/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|âââââââââââââââââââââââââââââââ          | 186/249 [10:54<01:18,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching URL: 403 Client Error: Forbidden for url: https://www.datacamp.com/tutorial\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|âââââââââââââââââââââââââââââââ          | 187/249 [10:56<01:21,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching URL: 404 Client Error: Not Found for url: https://sites.google.com/site/hwinteractionlab//E2E/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|ââââââââââââââââââââââââââââââââ         | 193/249 [11:14<02:35,  2.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching URL: HTTPSConnectionPool(host='webnlg-challenge.loria.fr', port=443): Max retries exceeded with url: /challenge_2017/ (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1007)')))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|âââââââââââââââââââââââââââââââââ        | 198/249 [11:24<01:37,  1.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching URL: HTTPSConnectionPool(host='blog.sdl.com', port=443): Max retries exceeded with url: /blog/understanding-mt-quality-bleu-scores.html (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x16a50d750>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|ââââââââââââââââââââââââââââââââââ       | 205/249 [11:41<01:49,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching URL: 403 Client Error: Forbidden for url: https://www.chatbotpack.com/design-conversational-interfaces\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|âââââââââââââââââââââââââââââââââââââ    | 222/249 [12:35<01:21,  3.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching URL: 403 Client Error: Forbidden for url: https://machinelearningmastery.com/what-is-deep-learning/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|ââââââââââââââââââââââââââââââââââââââ   | 226/249 [12:46<00:52,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching URL: HTTPSConnectionPool(host='www.nrybbs.top', port=443): Max retries exceeded with url: /products.aspx?cname=reinforcement+learning+machine+learning&cid=95 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x169b6c430>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|âââââââââââââââââââââââââââââââââââââââ  | 233/249 [13:05<00:41,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching URL: 403 Client Error: Forbidden for url: https://www.researchgate.net/figure/Illustration-of-the-BERT-model-for-joint-intent-detection-and-slot-filling_fig3_359661835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|ââââââââââââââââââââââââââââââââââââââââ | 239/249 [13:15<00:14,  1.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching URL: 403 Client Error: Forbidden for url: https://machinelearningmastery.com/a-gentle-introduction-to-positional-encoding-in-transformer-models-part-1/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|âââââââââââââââââââââââââââââââââââââââââ| 247/249 [13:36<00:03,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching URL: 403 Client Error: Forbidden for url: https://deepchecks.com/5-approaches-to-solve-llm-token-limits/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|âââââââââââââââââââââââââââââââââââââââââ| 249/249 [13:40<00:00,  3.30s/it]\n"
     ]
    }
   ],
   "source": [
    "# GET TEXT FROM URLS\n",
    "url_results = {}\n",
    "url_dataframe = pd.DataFrame(columns=['text', 'text_length','gram_score'])\n",
    "\n",
    "\n",
    "for url in tqdm(url_list):\n",
    "    \n",
    "    if is_pdf_url_headers(url):\n",
    "        pdf_urls.append(url)\n",
    "        continue\n",
    "    \n",
    "    article_texts = extract_text_from_url(url) # Input: URL; Output: Text retrieved from the link \n",
    "    chunks = chunk_text(article_texts)\n",
    "    \n",
    "    for t in chunks:\n",
    "        row = [t, len(t.split()) ,grammar_check(t)]\n",
    "        url_dataframe.loc[len(url_dataframe)] = row\n",
    "            \n",
    "        \n",
    "url_dataframe.to_csv(\"URL_text_300.csv\", index= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c797500c",
   "metadata": {},
   "source": [
    "### Extract textual data from URLs pointing to PDF files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3e8e383f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://arxiv.org/pdf/2205.06573.pdf',\n",
       " 'https://arxiv.org/pdf/2106.07447.pdf',\n",
       " 'https://www.speech.kth.se/~rolf/NGSLT/presentations/ASV.pdf',\n",
       " 'https://s3.amazonaws.com/assets.datacamp.com/production/course_3631/slides/chapter2.pdf',\n",
       " 'https://arxiv.org/pdf/2106.07447.pdf',\n",
       " 'https://cs.stanford.edu/~acoates/ba_dls_speech2016.pdf',\n",
       " 'https://people.kth.se/~ghe/pubs/pdf/wagner2019speech.pdf',\n",
       " 'https://nld.ict.usc.edu/cs644-spring2020/discussions/novikova-etal-emnlp2017.pdf']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "278bd3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts and returns text from URLs pointing towards PDF files. (Most likely research articles or reviews)\n",
    "def extract_text_from_pdf_url(pdf_url):\n",
    "\n",
    "    extracted_text = \" \"\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(pdf_url, timeout=5)\n",
    "        pdf_file = open('temp.pdf', 'wb')  # Save PDF temporarily\n",
    "        pdf_file.write(response.content)\n",
    "        pdf_file.close()\n",
    "\n",
    "        pdf_reader = PyPDF2.PdfReader('temp.pdf')\n",
    "        num_pages = len(pdf_reader.pages)\n",
    "\n",
    "        for page_num in range(num_pages):\n",
    "            page = pdf_reader.pages[page_num]\n",
    "            text = page.extract_text()\n",
    "            extracted_text += text\n",
    "            \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching URL: {e}\")\n",
    "        forbidden_urls.append(url)\n",
    "        extracted_text+= \" \"\n",
    "\n",
    "    except requests.exceptions.Timeout as e:\n",
    "        print(f\"Error fetching URL: {e}\")\n",
    "        forbidden_urls.append(url)\n",
    "        extracted_text+= \" \"\n",
    "    \n",
    "    finally:\n",
    "        if os.path.exists('temp.pdf'):  # Check if the file exists before deleting\n",
    "            os.remove('temp.pdf')\n",
    "    \n",
    "    return extracted_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "85c14447",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|âââââââââââââââââââââââââââââââââââââââââââââ| 8/8 [01:59<00:00, 14.88s/it]\n"
     ]
    }
   ],
   "source": [
    "PDF_url_dataframe = pd.DataFrame(columns=['text', 'text_length','gram_score'])\n",
    "\n",
    "for url in tqdm(pdf_urls):\n",
    "    \n",
    "    text = extract_text_from_pdf_url(url)\n",
    "    text = clean_text(text)\n",
    "    \n",
    "    chunks = chunk_text(text)\n",
    "    \n",
    "    for t in chunks:\n",
    "        row = [t, len(t.split()) ,grammar_check(t)]\n",
    "        PDF_url_dataframe.loc[len(PDF_url_dataframe)] = row\n",
    "        \n",
    "        \n",
    "PDF_url_dataframe.to_csv(\"PDF_urls_text_300.csv\", index= False, quoting=csv.QUOTE_NONE, escapechar=' ')        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35901dbb",
   "metadata": {},
   "source": [
    "### Additional Data Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e650c5d2",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "#### 1. Extract textual data from Wikipedia articles corresponding to keywords of the course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f8c647cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually gathered keywords that have an article related to it in correlation with Speech Processing\n",
    "# This process can be further automated using keyword extraction.\n",
    "\n",
    "queries = ['Virtual Assistant', 'Natural language generation' ,'Keyword spotting', 'Speech Recognition', \n",
    "           'Question answering', 'Speech Processing', 'Speech Synthesis', 'Voice activity detection',\n",
    "           'utterance', 'paraphrase', 'Prosody (linguistics)' , 'Connectionist temporal classification', \n",
    "           'Language model', 'representation learning', 'word embedding', 'large language model',\n",
    "          'Transformer (deep learning architecture)', 'Voice Analysis']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4083b2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wikipedia_content(search_query):\n",
    "\n",
    "    try:\n",
    "        result = wikipedia.search(search_query)[0]\n",
    "\n",
    "        # Fetch the full article content\n",
    "        page = wikipedia.page(result)\n",
    "        return page.content\n",
    "\n",
    "    except wikipedia.exceptions.DisambiguationError as e:\n",
    "        print(f\"Multiple results found: {e.options}\")\n",
    "        return None\n",
    "    except wikipedia.exceptions.PageError:\n",
    "        print(f\"Page not found: {search_query}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "05498283",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_keywords_wiki = {}\n",
    "\n",
    "for keyword in queries:\n",
    "    \n",
    "    content = get_wikipedia_content(keyword)\n",
    "    if content:\n",
    "        texts_keywords_wiki[keyword] = clean_text(content)\n",
    "    else:\n",
    "        print(\"Wikipedia article not found or an error occurred.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7ca36ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|âââââââââââââââââââââââââââââââââââââââââââ| 18/18 [01:01<00:00,  3.44s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "wiki_dataframe = pd.DataFrame(columns=['text', 'text_length','gram_score'])\n",
    "\n",
    "for KW, content in tqdm(texts_keywords_wiki.items()):\n",
    "    \n",
    "    chunks = chunk_text(content)\n",
    "    \n",
    "    for t in chunks:\n",
    "        row = [t, len(t.split()) ,grammar_check(t)]\n",
    "        wiki_dataframe.loc[len(wiki_dataframe)] = row\n",
    "        \n",
    "        \n",
    "wiki_dataframe.to_csv(\"Wiki_text_300.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93a787d",
   "metadata": {},
   "source": [
    "##### 2. Extract text from relevant chapters of the book, \"Speech and Language Processing.\" by Daniel Jurafsky & James H. Martin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aa7a446d",
   "metadata": {},
   "outputs": [],
   "source": [
    "book_folder = \"./Book Chapters/\"\n",
    "chapters_pdf_files = [f for f in os.listdir(book_folder) if f.endswith('.pdf')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "75260a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "for file_name in chapters_pdf_files:\n",
    "    \n",
    "    file_path = os.path.join(book_folder, file_name)\n",
    "    try:\n",
    "        text_data = extract_pdf_text(file_path)\n",
    "        combined_text = \" \".join(text_data)\n",
    "        results[file_name] = combined_text\n",
    "    except PyPDF2.errors.PdfReadError:\n",
    "        print(f\"Error processing file (pdferror): {file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "76ae5604",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "books_DF = pd.DataFrame(columns=['text', 'text_length','gram_score'])\n",
    "\n",
    "\n",
    "for texts in results.values():\n",
    "    chunks = chunk_text(texts)\n",
    "    \n",
    "    for t in chunks:\n",
    "        t = clean_text(t)\n",
    "        row = [t, len(t.split()) ,grammar_check(t)]\n",
    "        books_DF.loc[len(books_DF)] = row\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92136b47",
   "metadata": {},
   "source": [
    "##### 3. Using the online book provided by Aalto Univeristy titled, \"Introduction to Speech Processing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d1ff651a",
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_urls = ['https://speechprocessingbook.aalto.fi/Recognition_tasks_in_speech_processing.html',\n",
    "              'https://speechprocessingbook.aalto.fi/Recognition/Voice_activity_detection.html',\n",
    "             'https://speechprocessingbook.aalto.fi/Recognition/Speech_Recognition.html',\n",
    "             'https://speechprocessingbook.aalto.fi/Recognition/Speaker_Recognition_and_Verification.html',\n",
    "             'https://speechprocessingbook.aalto.fi/Recognition/Speaker_Diarization.html',\n",
    "             'https://speechprocessingbook.aalto.fi/Recognition/Paralinguistic_speech_processing.html',\n",
    "             'https://speechprocessingbook.aalto.fi/Speech_Synthesis.html',\n",
    "             'https://speechprocessingbook.aalto.fi/Synthesis/Concatenative_speech_synthesis.html',\n",
    "             'https://speechprocessingbook.aalto.fi/Synthesis/Statistical_parametric_speech_synthesis.html',\n",
    "             'https://speechprocessingbook.aalto.fi/Analysis/Forensic_speaker_recognition.html']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "938891a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|âââââââââââââââââââââââââââââââââââââââââââ| 10/10 [00:53<00:00,  5.37s/it]\n"
     ]
    }
   ],
   "source": [
    "# GET TEXT FROM URLS\n",
    "extra_url_results = {}\n",
    "extra_url_dataframe = pd.DataFrame(columns=['text', 'text_length','gram_score'])\n",
    "\n",
    "for url in tqdm(extra_urls):\n",
    "    \n",
    "    article_texts = extract_text_from_url(url) # Input: URL; Output: Text retrieved from the link \n",
    "    chunks = chunk_text(article_texts)\n",
    "    \n",
    "    for t in chunks:\n",
    "        row = [t, len(t.split()) ,grammar_check(t)]\n",
    "        extra_url_dataframe.loc[len(extra_url_dataframe)] = row\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0b04f953",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_DF = pd.concat([books_DF, extra_url_dataframe], ignore_index=True)\n",
    "new_DF.to_csv(\"Book_text_300.csv\", index=False, quoting=csv.QUOTE_NONE, escapechar=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1110bf29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text           a virtual assistant va is a software agent tha...\n",
       "text_length                                                  300\n",
       "gram_score                                                    20\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_dataframe.iloc[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1c6a55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
