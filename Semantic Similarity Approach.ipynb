{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63e7b945-b9f0-49f2-a3ee-37d6c71fdf40",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3711f794",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, set_seed\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from scipy.spatial.distance import cosine\n",
    "import seaborn as sns\n",
    "import re\n",
    "import torch\n",
    "import time\n",
    "import spacy\n",
    "import faiss\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "from nltk import ngrams \n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70137256-c1ce-452b-bac6-6bf934a38035",
   "metadata": {},
   "outputs": [],
   "source": [
    "access_token = \"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\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164dc599-7df4-4020-aabb-7310893e16f7",
   "metadata": {},
   "source": [
    "### Defining Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bcd93d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_closest_cluster(emb, cluster_dict):\n",
    "    min_distance = float(\"inf\")\n",
    "    closest_cluster_id = None\n",
    "\n",
    "    for cluster_id, cluster_center in cluster_dict.items():\n",
    "        distance = cosine(emb, cluster_center)\n",
    "        if distance < min_distance:\n",
    "            min_distance = distance\n",
    "            closest_cluster_id = cluster_id\n",
    "\n",
    "    return closest_cluster_id, min_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e64aebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevant_text_from_cluster(embedding, df, cluster_id, n):\n",
    "    df2 = df[df.cluster == cluster_id]\n",
    "    distances = []\n",
    "    for i in range(len(df2)):\n",
    "        # Calculate distance between embedding and each embedding in dataframe\n",
    "        distance = cosine(embedding, np.array(df2['embeddings'].iloc[i]))\n",
    "        distances.append((distance, i))\n",
    "    nearest_neighbors = sorted(distances, key=lambda x: x[0])[:n]\n",
    "\n",
    "    for distance, index in nearest_neighbors:\n",
    "        print(f\"Distance: {distance}, Index: {index}\")\n",
    "    return df2.iloc[[i[1] for i in nearest_neighbors]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e15bc41a-fcc0-45ad-8735-c12f8fd55e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevant_text(embedding, df, n, verbose=0, return_score=False):\n",
    "    df2 = df.copy()\n",
    "    distances = []\n",
    "    for i in range(len(df2)):\n",
    "        # Calculate distance between embedding and each embedding in dataframe\n",
    "        distance = cosine(embedding, np.array(df2['embeddings'].iloc[i]))\n",
    "        distances.append((distance, i))\n",
    "    nearest_neighbors = sorted(distances, key=lambda x: x[0])[:n]\n",
    "\n",
    "    if verbose > 0:\n",
    "        for distance, index in nearest_neighbors:\n",
    "            print(f\"Distance: {distance}, Index: {index}\")\n",
    "            \n",
    "    if return_score:\n",
    "        return df2.iloc[[i[1] for i in nearest_neighbors]], [i[0] for i in nearest_neighbors]\n",
    "        \n",
    "    return df2.iloc[[i[1] for i in nearest_neighbors]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5fd0afb0-48ad-404c-b144-3d9ddd20114f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_QA(text):\n",
    "    \"\"\" Extracts the Question and the Options from an MCQ \"\"\"\n",
    "    import re\n",
    "    pattern = r\"Question: (.*?)\\(\"  \n",
    "    result = re.search(pattern, text)\n",
    "    if result:\n",
    "        question = result.group(1).strip()  # Extract the captured question text\n",
    "    option_pattern = r\"\\(([a-zA-Z])\\) (.*?)(?=\\(|\\s*$)\"\n",
    "    option_matches = re.findall(option_pattern, text)\n",
    "    return {\"Question\": question, \"Options\": [o[1] for o in option_matches]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c97748cf-03a2-48c7-bdec-a0a60077f53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.lemma_.lower() for token in doc if not token.is_stop]\n",
    "    bigrams = ngrams(tokens, 2)\n",
    "    trigrams = ngrams(tokens, 3)\n",
    "    return tokens + ['_'.join(bg) for bg in bigrams] +  ['_'.join(tg) for tg in trigrams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f465f953-b90c-488b-a69f-67a0a21e3e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Indexing with Faiss\n",
    "def build_index_with_faiss(corpus):\n",
    "    tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "    dimension = tfidf_matrix.shape[1]\n",
    "    index = faiss.IndexFlatL2(dimension)  # Simple L2 distance index\n",
    "    index.add(tfidf_matrix.toarray()) \n",
    "    return index, vectorizer  # Return vectorizer for query processing\n",
    "\n",
    "def retrieve_documents(query, index, vectorizer, k=5):  \n",
    "    query_vector = vectorizer.transform([\" \".join(preprocess(query))]).toarray()\n",
    "    distances, indices = index.search(query_vector, k) \n",
    "    pd.DataFrame([distances[0], indices[0]])\n",
    "    return indices[0]  # Return the top K document indices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22239707-d9aa-493c-b3f0-be5c5a15d867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Question Answering\n",
    "def answer_question(query, inverted_index, corpus, vectorizer, n=5):\n",
    "    query_preprocessed = preprocess(query)\n",
    "    query_vector = vectorizer.transform([\" \".join(query_preprocessed)]).toarray()[0]\n",
    "    \n",
    "    top_doc_indices = retrieve_documents(query, inverted_index, vectorizer)\n",
    "    results = [corpus[idx] for idx in top_doc_indices]  \n",
    "\n",
    "    # Sentence Ranking (With N-grams and TF-IDF)\n",
    "    ranked_sentences = []\n",
    "    for doc in results:\n",
    "        sentences = nlp(doc).sents\n",
    "        for sent in sentences:\n",
    "            sent_preprocessed = preprocess(sent.text) \n",
    "            sent_vector = vectorizer.transform([\" \".join(sent_preprocessed)]).toarray()[0]\n",
    "            similarity = np.dot(query_vector, sent_vector) / (np.linalg.norm(query_vector) * np.linalg.norm(sent_vector))\n",
    "            ranked_sentences.append((sent.text, similarity))\n",
    "\n",
    "    ranked_sentences = sorted(ranked_sentences, key=lambda x: x[1], reverse=True)\n",
    "    top_sentences = [sent[0] for sent in ranked_sentences[:n]]\n",
    "\n",
    "    return top_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "709df3e7-a309-4459-a20d-0d70bc46a1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_context(retrieved_texts, scores, threshold, verbose=0, baseline=False):\n",
    "        if baseline:\n",
    "            retrieved_texts = retrieved_texts\n",
    "        else:\n",
    "            retrieved_texts = retrieved_texts.copy().text.to_list()\n",
    "        relevant_text = []\n",
    "        for i in range(len(scores)):\n",
    "            if verbose > 0:\n",
    "                print(score[i])\n",
    "            if score[i] < 0.6:\n",
    "                relevant_text.append(retrieved_texts[i])\n",
    "        return relevant_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eacce351-b251-4069-a793-9f24f741a800",
   "metadata": {},
   "source": [
    "Set a seed value for randomisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a65b131-b0e7-424c-a8e0-979ffeb280c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 37"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "71c000c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "157e8ffd-9115-4a6e-9b38-dc6a27448311",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53af29d5-f281-4005-8968-7a51659eaca6",
   "metadata": {},
   "source": [
    "### Loading and processing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0970d50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_text = pd.read_csv(\"PDF_file_text_300.csv\").drop_duplicates()\n",
    "url_text = pd.read_csv(\"URL_text_300.csv\").drop_duplicates()\n",
    "pdf_url_text = pd.read_csv(\"PDF_urls_text_300.csv\").drop_duplicates()\n",
    "wiki_text = pd.read_csv(\"wiki_text_300.csv\").drop_duplicates()\n",
    "new_text = pd.read_csv(\"book_text_300.csv\").drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "17b4f41b-c1d8-45b3-a1f0-53ce84b8400b",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_text = pd.concat([pdf_text, url_text, pdf_url_text, wiki_text, new_text]).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c3fb955a-6e5e-4f8d-ba12-15e3429d6b88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_length</th>\n",
       "      <th>gram_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1605.000000</td>\n",
       "      <td>1605.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>280.152025</td>\n",
       "      <td>30.694704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>61.389323</td>\n",
       "      <td>18.245151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>300.000000</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>300.000000</td>\n",
       "      <td>27.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>300.000000</td>\n",
       "      <td>38.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>300.000000</td>\n",
       "      <td>191.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       text_length   gram_score\n",
       "count  1605.000000  1605.000000\n",
       "mean    280.152025    30.694704\n",
       "std      61.389323    18.245151\n",
       "min       2.000000     1.000000\n",
       "25%     300.000000    19.000000\n",
       "50%     300.000000    27.000000\n",
       "75%     300.000000    38.000000\n",
       "max     300.000000   191.000000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_text.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "49b78d28-4a5c-43de-b2d5-0051c63bcfef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='gram_score', ylabel='Count'>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkcAAAGwCAYAAACjPMHLAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/H5lhTAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAyAUlEQVR4nO3deXQUZb7/8U+HpdmyGEI2DQmgLCr7ksugCBKB4KAOjAriiMqAyiZkxsEoyDJzb1AUuDAIzvxYnCOIegRUVJQdlYBskUEhl2Qia5YBbhISJGR5fn/4o3/VZoPQ6U4n79c5dU7qeaqqvw+V5UPV09U2Y4wRAAAAJEk+ni4AAACgJiEcAQAAWBCOAAAALAhHAAAAFoQjAAAAC8IRAACABeEIAADAor6nC6gJSkpKdPbsWfn6+spms3m6HAAAcA2MMbp48aLCw8Pl4+O66z2EI0lnz55VRESEp8sAAABVcOrUKd1yyy0uOx7hSJKvr6+kn/9x/fz8PFwNAAC4Frm5uYqIiHD8HXcVwpHkuJXm5+dHOAIAwMu4ekoME7IBAAAsCEcAAAAWhCMAAAALwhEAAIAF4QgAAMCCcAQAAGBBOAIAALDwaDjatWuXhg4dqvDwcNlsNm3YsMGp32azlbnMmzfPsU1UVFSp/rlz57p5JAAAoLbwaDjKz89X586dtWTJkjL709PTnZYVK1bIZrNp+PDhTtvNmTPHabtJkya5o3wAAFALefQJ2bGxsYqNjS23PzQ01Gn9o48+Uv/+/dW6dWundl9f31LbAgAAVIXXzDnKzMzUp59+qjFjxpTqmzt3rpo3b66uXbtq3rx5KioqqvBYBQUFys3NdVoAAAAkL/pstbffflu+vr4aNmyYU/vkyZPVrVs3BQYGavfu3YqPj1d6errmz59f7rESEhI0e/bs6i4ZAAB4IZsxxni6COnnydfr16/XQw89VGZ/+/btdd9992nx4sUVHmfFihV65plnlJeXJ7vdXuY2BQUFKigocKxf/VTfnJwcPngWAAAvkZubK39/f5f//faKK0dfffWVkpOT9d5771W6bXR0tIqKivTjjz+qXbt2ZW5jt9vLDU4AAKBu84o5R8uXL1f37t3VuXPnSrdNSkqSj4+PgoOD3VAZAACobTx65SgvL08pKSmO9bS0NCUlJSkwMFAtW7aU9PMlsw8++EBvvPFGqf0TExO1d+9e9e/fX76+vkpMTNTUqVP1+OOP66abbnLbOGqq/gNjlZ51rsy+sOAgbf/yczdXBABAzefRcLR//37179/fsR4XFydJGj16tFatWiVJWrt2rYwxGjlyZKn97Xa71q5dq1mzZqmgoECtWrXS1KlTHcep69KzzqnL+EVl9iW9OdnN1QAA4B08Go769eunyuaDjxs3TuPGjSuzr1u3btqzZ091lAYAAOoor5hzBAAA4C6EIwAAAAvCEQAAgIVXPOcItUtF76KTeCcdAMCzCEdwu4reRSfxTjoAgGdxWw0AAMCCcAQAAGBBOAIAALAgHAEAAFgQjgAAACwIRwAAABaEIwAAAAvCEQAAgAXhCAAAwIJwBAAAYEE4AgAAsCAcAQAAWBCOAAAALAhHAAAAFoQjAAAAC8IRAACABeEIAADAgnAEAABgQTgCAACwIBwBAABYEI4AAAAsCEcAAAAW9T1dALxT/4GxSs86V2ZfWHCQtn/5uZsrAgDANQhHqJL0rHPqMn5RmX1Jb052czUAALgOt9UAAAAsCEcAAAAWhCMAAAALwhEAAIAF4QgAAMCCcAQAAGBBOAIAALAgHAEAAFgQjgAAACwIRwAAABaEIwAAAAuPhqNdu3Zp6NChCg8Pl81m04YNG5z6n3zySdlsNqdl8ODBTttcuHBBo0aNkp+fnwICAjRmzBjl5eW5cRQAAKA28egHz+bn56tz5856+umnNWzYsDK3GTx4sFauXOlYt9vtTv2jRo1Senq6Nm/erMLCQj311FMaN26c1qxZU62113b9B8YqPetcuf2nz5xVF/eVAwCA23g0HMXGxio2NrbCbex2u0JDQ8vsO3r0qDZt2qR9+/apR48ekqTFixdryJAhev311xUeHu7ymuuK9Kxz6jJ+Ubn9J178jRurAQDAfWr8nKMdO3YoODhY7dq103PPPafz5887+hITExUQEOAIRpIUExMjHx8f7d27t9xjFhQUKDc312kBAACQang4Gjx4sP7xj39o69atevXVV7Vz507FxsaquLhYkpSRkaHg4GCnferXr6/AwEBlZGSUe9yEhAT5+/s7loiIiGodBwAA8B4eva1WmREjRji+7tixozp16qQ2bdpox44dGjBgQJWPGx8fr7i4OMd6bm4uAQkAAEiq4eHol1q3bq2goCClpKRowIABCg0NVVZWltM2RUVFunDhQrnzlKSf5zH9cmJ3XXP61Cm179Kz/H4mXAMA6iivCkenT5/W+fPnFRYWJknq3bu3srOzdeDAAXXv3l2StG3bNpWUlCg6OtqTpdZ4xUZMuAYAoAweDUd5eXlKSUlxrKelpSkpKUmBgYEKDAzU7NmzNXz4cIWGhio1NVV/+tOfdOutt2rQoEGSpA4dOmjw4MEaO3asli1bpsLCQk2cOFEjRozgnWoAAKBKPDohe//+/eratau6du0qSYqLi1PXrl31yiuvqF69ejp8+LAeeOABtW3bVmPGjFH37t311VdfOd0SW716tdq3b68BAwZoyJAhuuuuu/S3v/3NU0MCAABezqNXjvr16ydjTLn9X3zxRaXHCAwM5IGPNQzzmQAA3syr5hzBOzCfCQDgzWr0c44AAADcjXAEAABgQTgCAACwIBwBAABYEI4AAAAsCEcAAAAWhCMAAAALwhEAAIAF4QgAAMCCcAQAAGBBOAIAALAgHAEAAFgQjgAAACwIRwAAABaEIwAAAAvCEQAAgAXhCAAAwIJwBAAAYEE4AgAAsCAcAQAAWBCOAAAALAhHAAAAFoQjAAAAC8IRAACABeEIAADAgnAEAABgQTgCAACwIBwBAABYEI4AAAAsCEcAAAAWhCMAAAALwhEAAIAF4QgAAMCCcAQAAGBBOAIAALAgHAEAAFgQjgAAACwIRwAAABaEIwAAAAuPhqNdu3Zp6NChCg8Pl81m04YNGxx9hYWFmjZtmjp27KimTZsqPDxcTzzxhM6ePet0jKioKNlsNqdl7ty5bh4JAACoLTwajvLz89W5c2ctWbKkVN+lS5d08OBBzZgxQwcPHtS6deuUnJysBx54oNS2c+bMUXp6umOZNGmSO8oHAAC1UH1PvnhsbKxiY2PL7PP399fmzZud2v7617+qV69eOnnypFq2bOlo9/X1VWho6DW/bkFBgQoKChzrubm511k5AACorbxqzlFOTo5sNpsCAgKc2ufOnavmzZura9eumjdvnoqKiio8TkJCgvz9/R1LRERENVYNAAC8iUevHF2Py5cva9q0aRo5cqT8/Pwc7ZMnT1a3bt0UGBio3bt3Kz4+Xunp6Zo/f365x4qPj1dcXJxjPTc3l4AEAAAkeUk4Kiws1COPPCJjjJYuXerUZw05nTp1UsOGDfXMM88oISFBdru9zOPZ7fZy+wAAQN1W42+rXQ1GJ06c0ObNm52uGpUlOjpaRUVF+vHHH91TIAAAqFVq9JWjq8Ho+PHj2r59u5o3b17pPklJSfLx8VFwcLAbKgQAALWNR8NRXl6eUlJSHOtpaWlKSkpSYGCgwsLC9Nvf/lYHDx7Uxo0bVVxcrIyMDElSYGCgGjZsqMTERO3du1f9+/eXr6+vEhMTNXXqVD3++OO66aabPDUsAADgxTwajvbv36/+/fs71q/OHxo9erRmzZqljz/+WJLUpUsXp/22b9+ufv36yW63a+3atZo1a5YKCgrUqlUrTZ061WkeEgAAwPXwaDjq16+fjDHl9lfUJ0ndunXTnj17XF0WAACow2r8hGwAAAB3IhwBAABYEI4AAAAsCEcAAAAWhCMAAAALwhEAAIAF4QgAAMCCcAQAAGBBOAIAALAgHAEAAFgQjgAAACwIRwAAABaEIwAAAAvCEQAAgAXhCAAAwIJwBAAAYEE4AgAAsCAcAQAAWBCOAAAALAhHAAAAFoQjAAAAC8IRAACABeEIAADAgnAEAABgQTgCAACwIBwBAABYEI4AAAAsCEcAAAAWhCMAAAALwhEAAIAF4QgAAMCCcAQAAGBBOAIAALAgHAEAAFgQjgAAACwIRwAAABaEIwAAAAvCEQAAgAXhCAAAwIJwBAAAYOHRcLRr1y4NHTpU4eHhstls2rBhg1O/MUavvPKKwsLC1LhxY8XExOj48eNO21y4cEGjRo2Sn5+fAgICNGbMGOXl5blxFAAAoDbxaDjKz89X586dtWTJkjL7X3vtNS1atEjLli3T3r171bRpUw0aNEiXL192bDNq1Ch9//332rx5szZu3Khdu3Zp3Lhx7hoCAACoZep78sVjY2MVGxtbZp8xRgsXLtT06dP14IMPSpL+8Y9/KCQkRBs2bNCIESN09OhRbdq0Sfv27VOPHj0kSYsXL9aQIUP0+uuvKzw8vMxjFxQUqKCgwLGem5vr4pEBAABvVWPnHKWlpSkjI0MxMTGONn9/f0VHRysxMVGSlJiYqICAAEcwkqSYmBj5+Pho79695R47ISFB/v7+jiUiIqL6BgIAALxKlcJR69atdf78+VLt2dnZat269Q0XJUkZGRmSpJCQEKf2kJAQR19GRoaCg4Od+uvXr6/AwEDHNmWJj49XTk6OYzl16pRLagYAAN6vSrfVfvzxRxUXF5dqLygo0JkzZ264qOpmt9tlt9s9XQYAAKiBriscffzxx46vv/jiC/n7+zvWi4uLtXXrVkVFRbmksNDQUElSZmamwsLCHO2ZmZnq0qWLY5usrCyn/YqKinThwgXH/gAAANfjusLRQw89JEmy2WwaPXq0U1+DBg0UFRWlN954wyWFtWrVSqGhodq6dasjDOXm5mrv3r167rnnJEm9e/dWdna2Dhw4oO7du0uStm3bppKSEkVHR7ukDgAAULdcVzgqKSmR9HNw2bdvn4KCgm7oxfPy8pSSkuJYT0tLU1JSkgIDA9WyZUtNmTJFf/nLX3TbbbepVatWmjFjhsLDwx0hrUOHDho8eLDGjh2rZcuWqbCwUBMnTtSIESPKfacaAABARao05ygtLc0lL75//37179/fsR4XFydJGj16tFatWqU//elPys/P17hx45Sdna277rpLmzZtUqNGjRz7rF69WhMnTtSAAQPk4+Oj4cOHa9GiRS6pDwAA1D1Vfs7R1q1btXXrVmVlZTmuKF21YsWKazpGv379ZIwpt99ms2nOnDmaM2dOudsEBgZqzZo111Y0AABAJaoUjmbPnq05c+aoR48eCgsLk81mc3VdAAAAHlGlcLRs2TKtWrVKv/vd71xdDwAAgEdV6SGQV65c0a9+9StX1wIAAOBxVQpHv//975nnAwAAaqUq3Va7fPmy/va3v2nLli3q1KmTGjRo4NQ/f/58lxQHAADgblUKR4cPH3Y8mPHIkSNOfUzOBgAA3qxK4Wj79u2urgMAAKBGqNKcIwAAgNqqSleO+vfvX+Hts23btlW5IAAAAE+qUji6Ot/oqsLCQiUlJenIkSOlPpAWAADAm1QpHC1YsKDM9lmzZikvL++GCgIAAPAkl845evzxx6/5c9UAAABqIpeGo8TERDVq1MiVhwQAAHCrKt1WGzZsmNO6MUbp6enav3+/ZsyY4ZLCAAAAPKFK4cjf399p3cfHR+3atdOcOXM0cOBAlxQGAADgCVUKRytXrnR1HQAAADVClcLRVQcOHNDRo0clSXfccYe6du3qkqIAAAA8pUrhKCsrSyNGjNCOHTsUEBAgScrOzlb//v21du1atWjRwpU1AgAAuE2V3q02adIkXbx4Ud9//70uXLigCxcu6MiRI8rNzdXkyZNdXSMAAIDbVOnK0aZNm7RlyxZ16NDB0Xb77bdryZIlTMgGAABerUpXjkpKStSgQYNS7Q0aNFBJSckNFwUAAOApVQpH9957r55//nmdPXvW0XbmzBlNnTpVAwYMcFlxAAAA7lalcPTXv/5Vubm5ioqKUps2bdSmTRu1atVKubm5Wrx4satrBAAAcJsqzTmKiIjQwYMHtWXLFh07dkyS1KFDB8XExLi0OOCX+g+MVXrWuXL7w4KDtP3Lz91YEQCgtrmucLRt2zZNnDhRe/bskZ+fn+677z7dd999kqScnBzdcccdWrZsme6+++5qKRZIzzqnLuMXlduf9CbvlgQA3Jjruq22cOFCjR07Vn5+fqX6/P399cwzz2j+/PkuKw4AAMDdriscfffddxo8eHC5/QMHDtSBAwduuCgAAABPua5wlJmZWeZb+K+qX7++/v3vf99wUQAAAJ5yXeHo5ptv1pEjR8rtP3z4sMLCwm64KAAAAE+5rgnZQ4YM0YwZMzR48GA1atTIqe+nn37SzJkz9etf/9qlBaLuOX3qlNp36Vl235mz6uLecgAAdcx1haPp06dr3bp1atu2rSZOnKh27dpJko4dO6YlS5aouLhYL7/8crUUirqj2Kjcd6SdePE3bq4GAFDXXFc4CgkJ0e7du/Xcc88pPj5exhhJks1m06BBg7RkyRKFhIRUS6EAAADucN0PgYyMjNRnn32m//3f/1VKSoqMMbrtttt00003VUd9AAAAblWlJ2RL0k033aSePcueFwIAAOCtqvTZagAAALUV4QgAAMCCcAQAAGBBOAIAALAgHAEAAFjU+HAUFRUlm81WapkwYYIkqV+/fqX6nn32WQ9XDQAAvFWV38rvLvv27VNxcbFj/ciRI7rvvvv08MMPO9rGjh2rOXPmONabNGni1hoBAEDtUePDUYsWLZzW586dqzZt2uiee+5xtDVp0kShoaHuLg0AANRCNf62mtWVK1f0zjvv6Omnn5bNZnO0r169WkFBQbrzzjsVHx+vS5cuVXicgoIC5ebmOi0AAACSF1w5stqwYYOys7P15JNPOtoee+wxRUZGKjw8XIcPH9a0adOUnJysdevWlXuchIQEzZ492w0VAwAAb+NV4Wj58uWKjY1VeHi4o23cuHGOrzt27KiwsDANGDBAqampatOmTZnHiY+PV1xcnGM9NzdXERER1Vc4AADwGl4Tjk6cOKEtW7ZUeEVIkqKjoyVJKSkp5YYju90uu93u8hoBAID385o5RytXrlRwcLDuv//+CrdLSkqSJIWFhbmhKgAAUNt4xZWjkpISrVy5UqNHj1b9+v+/5NTUVK1Zs0ZDhgxR8+bNdfjwYU2dOlV9+/ZVp06dPFgxAADwVl4RjrZs2aKTJ0/q6aefdmpv2LChtmzZooULFyo/P18REREaPny4pk+f7qFKAQCAt/OKcDRw4EAZY0q1R0REaOfOnR6oCAAA1FZeM+cIAADAHQhHAAAAFoQjAAAAC8IRAACABeEIAADAgnAEAABgQTgCAACw8IrnHAHX6vSpU2rfpWe5/WHBQdr+5edurAgA4G0IR6hVio3UZfyicvuT3pzsxmoAAN6I22oAAAAWhCMAAAALwhEAAIAF4QgAAMCCcAQAAGBBOAIAALAgHAEAAFgQjgAAACwIRwAAABaEIwAAAAvCEQAAgAXhCAAAwIJwBAAAYEE4AgAAsCAcAQAAWNT3dAGouv4DY5Weda7c/tNnzqqL+8oBAKBWIBx5sfSsc+oyflG5/Sde/I0bqwEAoHbgthoAAIAF4QgAAMCCcAQAAGBBOAIAALAgHAEAAFgQjgAAACwIRwAAABaEIwAAAAseAok65fSpU2rfpWeZfWHBQdr+5edurggAUNMQjlCnFBuV+1TxpDcnu7kaAEBNxG01AAAAC8IRAACABeEIAADAgnAEAABgUaPD0axZs2Sz2ZyW9u3bO/ovX76sCRMmqHnz5mrWrJmGDx+uzMxMD1YMAAC8XY0OR5J0xx13KD093bF8/fXXjr6pU6fqk08+0QcffKCdO3fq7NmzGjZsmAerBQAA3q7Gv5W/fv36Cg0NLdWek5Oj5cuXa82aNbr33nslSStXrlSHDh20Z88e/cd//Ie7SwUAALVAjb9ydPz4cYWHh6t169YaNWqUTp48KUk6cOCACgsLFRMT49i2ffv2atmypRITEys8ZkFBgXJzc50WAAAAqYaHo+joaK1atUqbNm3S0qVLlZaWprvvvlsXL15URkaGGjZsqICAAKd9QkJClJGRUeFxExIS5O/v71giIiKqcRQAAMCb1OjbarGxsY6vO3XqpOjoaEVGRur9999X48aNq3zc+Ph4xcXFOdZzc3MJSAAAQFINv3L0SwEBAWrbtq1SUlIUGhqqK1euKDs722mbzMzMMucoWdntdvn5+TktAAAAkpeFo7y8PKWmpiosLEzdu3dXgwYNtHXrVkd/cnKyTp48qd69e3uwSgAA4M1q9G21P/7xjxo6dKgiIyN19uxZzZw5U/Xq1dPIkSPl7++vMWPGKC4uToGBgfLz89OkSZPUu3dv3qkGAACqrEaHo9OnT2vkyJE6f/68WrRoobvuukt79uxRixYtJEkLFiyQj4+Phg8froKCAg0aNEhvvvmmh6sGAADerEaHo7Vr11bY36hRIy1ZskRLlixxU0UAAKC286o5RwAAANWNcAQAAGBBOAIAALAgHAEAAFgQjgAAACwIRwAAABaEIwAAAIsa/ZwjwJ1Onzql9l16ltsfFhyk7V9+7saKAACeQDgC/p9iI3UZv6jc/qQ3J7uxGgCAp3BbDQAAwIJwBAAAYEE4AgAAsCAcAQAAWBCOAAAALAhHAAAAFoQjAAAAC8IRAACABeEIAADAgnAEAABgQTgCAACwIBwBAABYEI4AAAAsCEcAAAAWhCMAAAALwhEAAIAF4QgAAMCCcAQAAGBBOAIAALCo7+kCarv+A2OVnnWu3P6w4CBt//JzN1YEAAAqQjiqZulZ59Rl/KJy+5PenOzGagAAQGW4rQYAAGBBOAIAALAgHAEAAFgQjgAAACwIRwAAABaEIwAAAAvCEQAAgAXhCAAAwIKHQAI1QEVPUucp6gDgXoQjoAao6EnqPEUdANyrRoejhIQErVu3TseOHVPjxo31q1/9Sq+++qratWvn2KZfv37auXOn037PPPOMli1b5u5yXa6yz2U7feasurivnDrv9KlTat+lZ5l9XN0BgNqjRoejnTt3asKECerZs6eKior00ksvaeDAgfrhhx/UtGlTx3Zjx47VnDlzHOtNmjTxRLkuV9nnsp148TdurAbFRlzdAYA6oEaHo02bNjmtr1q1SsHBwTpw4ID69u3raG/SpIlCQ0PdXR7gUNFVJYkrSwDgTWp0OPqlnJwcSVJgYKBT++rVq/XOO+8oNDRUQ4cO1YwZMyq8elRQUKCCggLHem5ubvUUjDqjoqtKEleWAMCbeE04Kikp0ZQpU9SnTx/deeedjvbHHntMkZGRCg8P1+HDhzVt2jQlJydr3bp15R4rISFBs2fPdkfZAADAy3hNOJowYYKOHDmir7/+2ql93Lhxjq87duyosLAwDRgwQKmpqWrTpk2Zx4qPj1dcXJxjPTc3VxEREdVTOAAA8CpeEY4mTpyojRs3ateuXbrlllsq3DY6OlqSlJKSUm44stvtstvtLq8TAAB4vxodjowxmjRpktavX68dO3aoVatWle6TlJQkSQoLC6vm6gAAQG1Uo8PRhAkTtGbNGn300Ufy9fVVRkaGJMnf31+NGzdWamqq1qxZoyFDhqh58+Y6fPiwpk6dqr59+6pTp04erh4AAHijGh2Oli5dKunnBz1arVy5Uk8++aQaNmyoLVu2aOHChcrPz1dERISGDx+u6dOne6BaAABQG9TocGSMqbA/IiKi1NOxvU1Fz8fhCdgAALhfjQ5HdUFFz8fhCdgAALifj6cLAAAAqEkIRwAAABaEIwAAAAvCEQAAgAXhCAAAwIJwBAAAYEE4AgAAsCAcAQAAWBCOAAAALAhHAAAAFoQjAAAAC8IRAACABR88C9Rwp0+dUvsuPcvtDwsO0vYvP3djRQBQuxGOgBqu2Ehdxi8qtz/pzclurAYAaj/CEVCH9R8Yq/Ssc+X2c1UKQF1EOALqsPSsc1yVAoBfYEI2AACABeEIAADAgttqgBtU9o6z02fOqks1vXZF84qq83UBwFsRjgA3qOwdZyde/E21vXZF84qq83UBwFtxWw0AAMCCcAQAAGBBOAIAALAgHAEAAFgQjgAAACwIRwAAABa8lR/wcp58hhIA1EaEI8DLefIZSgBQGxGOAFSLip7MLUlhwUHa/uXnbqwIAK4N4QhAtajoydySlPTmZDdWAwDXjgnZAAAAFlw5AlBlfKgtgNqIcASgyvhQWwC1EeEIQI1U0VUpJnMDqE6EIwA1UkVXpZjMDaA6EY4AlIsHTAKoiwhHAMrFAyYB1EWEIwC1zo3MV+LhlQAIRwBqnRuZr8TDKwHUmnC0ZMkSzZs3TxkZGercubMWL16sXr16ebosANXgRuZCMY+qNK6W4VrUpXeQ1opw9N577ykuLk7Lli1TdHS0Fi5cqEGDBik5OVnBwcGeLg+Ai93IXCjmUZXG1TJci7r0DtJaEY7mz5+vsWPH6qmnnpIkLVu2TJ9++qlWrFihF1980cPVASgLV3Cujyev7lR2rmrbVQNcv9r2PeL14ejKlSs6cOCA4uPjHW0+Pj6KiYlRYmJimfsUFBSooKDAsZ6TkyNJys3NdXl9xcXFKvwpv9x+Y0rK7a+o70b7OXbN2beuHruopER3PJVQbv+Psx6rkeequLi4Wn5XVOZ0eqY6/f61cvsP/58/Vbmuyn5PVXaubuS14T0q+j7x1PfI1WMaY1x7YOPlzpw5YySZ3bt3O7W/8MILplevXmXuM3PmTCOJhYWFhYWFpRYsp06dcmm28PorR1URHx+vuLg4x3pJSYkuXLig5s2by2az3fDxc3NzFRERoVOnTsnPz++Gj1eTMdbaqy6Nl7HWXnVpvHVxrCdPnpTNZlN4eLhLj+/14SgoKEj16tVTZmamU3tmZqZCQ0PL3Mdut8tutzu1BQQEuLw2Pz+/Wv8NehVjrb3q0ngZa+1Vl8Zbl8bq7+9fLWP1cfkR3axhw4bq3r27tm7d6mgrKSnR1q1b1bt3bw9WBgAAvJHXXzmSpLi4OI0ePVo9evRQr169tHDhQuXn5zvevQYAAHCtakU4evTRR/Xvf/9br7zyijIyMtSlSxdt2rRJISEhHqnHbrdr5syZpW7d1UaMtfaqS+NlrLVXXRovY3UdmzGufv8bAACA9/L6OUcAAACuRDgCAACwIBwBAABYEI4AAAAsCEcutmTJEkVFRalRo0aKjo7Wt99+6+mSblhCQoJ69uwpX19fBQcH66GHHlJycrLTNv369ZPNZnNann32WQ9VfGNmzZpVaizt27d39F++fFkTJkxQ8+bN1axZMw0fPrzUQ0i9RVRUVKmx2mw2TZgwQZL3n9ddu3Zp6NChCg8Pl81m04YNG5z6jTF65ZVXFBYWpsaNGysmJkbHjx932ubChQsaNWqU/Pz8FBAQoDFjxigvL8+No7g2FY21sLBQ06ZNU8eOHdW0aVOFh4friSee0NmzZ52OUdb3w9y5c908kspVdl6ffPLJUuMYPHiw0zbecl6lysdb1s+wzWbTvHnzHNt4w7m9lr811/L79+TJk7r//vvVpEkTBQcH64UXXlBRUdF11UI4cqH33ntPcXFxmjlzpg4ePKjOnTtr0KBBysrK8nRpN2Tnzp2aMGGC9uzZo82bN6uwsFADBw5Ufr7zBxCOHTtW6enpjuW118r/kMya7o477nAay9dff+3omzp1qj755BN98MEH2rlzp86ePathw4Z5sNqq27dvn9M4N2/eLEl6+OGHHdt483nNz89X586dtWTJkjL7X3vtNS1atEjLli3T3r171bRpUw0aNEiXL192bDNq1Ch9//332rx5szZu3Khdu3Zp3Lhx7hrCNatorJcuXdLBgwc1Y8YMHTx4UOvWrVNycrIeeOCBUtvOmTPH6XxPmjTJHeVfl8rOqyQNHjzYaRzvvvuuU7+3nFep8vFax5menq4VK1bIZrNp+PDhTtvV9HN7LX9rKvv9W1xcrPvvv19XrlzR7t279fbbb2vVqlV65ZVXrq8Yl35SWx3Xq1cvM2HCBMd6cXGxCQ8PNwkJCR6syvWysrKMJLNz505H2z333GOef/55zxXlQjNnzjSdO3cusy87O9s0aNDAfPDBB462o0ePGkkmMTHRTRVWn+eff960adPGlJSUGGNq13mVZNavX+9YLykpMaGhoWbevHmOtuzsbGO32827775rjDHmhx9+MJLMvn37HNt8/vnnxmazmTNnzrit9uv1y7GW5dtvvzWSzIkTJxxtkZGRZsGCBdVbnIuVNdbRo0ebBx98sNx9vPW8GnNt5/bBBx809957r1ObN57bX/6tuZbfv5999pnx8fExGRkZjm2WLl1q/Pz8TEFBwTW/NleOXOTKlSs6cOCAYmJiHG0+Pj6KiYlRYmKiBytzvZycHElSYGCgU/vq1asVFBSkO++8U/Hx8bp06ZInynOJ48ePKzw8XK1bt9aoUaN08uRJSdKBAwdUWFjodJ7bt2+vli1bev15vnLlit555x09/fTTTh/AXJvOq1VaWpoyMjKczqW/v7+io6Md5zIxMVEBAQHq0aOHY5uYmBj5+Pho7969bq/ZlXJycmSz2Up9ruTcuXPVvHlzde3aVfPmzbvu2xE1xY4dOxQcHKx27drpueee0/nz5x19tfm8ZmZm6tNPP9WYMWNK9Xnbuf3l35pr+f2bmJiojh07Oj0EetCgQcrNzdX3339/za9dK56QXROcO3dOxcXFpZ7KHRISomPHjnmoKtcrKSnRlClT1KdPH915552O9scee0yRkZEKDw/X4cOHNW3aNCUnJ2vdunUerLZqoqOjtWrVKrVr107p6emaPXu27r77bh05ckQZGRlq2LBhqT8oISEhysjI8EzBLrJhwwZlZ2frySefdLTVpvP6S1fPV1k/s1f7MjIyFBwc7NRfv359BQYGevX5vnz5sqZNm6aRI0c6fWjn5MmT1a1bNwUGBmr37t2Kj49Xenq65s+f78Fqr9/gwYM1bNgwtWrVSqmpqXrppZcUGxurxMRE1atXr9aeV0l6++235evrW+pWv7ed27L+1lzL79+MjIwyf6av9l0rwhGuy4QJE3TkyBGnOTiSnO7Vd+zYUWFhYRowYIBSU1PVpk0bd5d5Q2JjYx1fd+rUSdHR0YqMjNT777+vxo0be7Cy6rV8+XLFxsYqPDzc0Vabzit+VlhYqEceeUTGGC1dutSpLy4uzvF1p06d1LBhQz3zzDNKSEjwqo+kGDFihOPrjh07qlOnTmrTpo127NihAQMGeLCy6rdixQqNGjVKjRo1cmr3tnNb3t8ad+G2mosEBQWpXr16pWbNZ2ZmKjQ01ENVudbEiRO1ceNGbd++XbfcckuF20ZHR0uSUlJS3FFatQoICFDbtm2VkpKi0NBQXblyRdnZ2U7bePt5PnHihLZs2aLf//73FW5Xm87r1fNV0c9saGhoqTdUFBUV6cKFC155vq8GoxMnTmjz5s1OV43KEh0draKiIv3444/uKbCatG7dWkFBQY7v29p2Xq/66quvlJycXOnPsVSzz215f2uu5fdvaGhomT/TV/uuFeHIRRo2bKju3btr69atjraSkhJt3bpVvXv39mBlN84Yo4kTJ2r9+vXatm2bWrVqVek+SUlJkqSwsLBqrq765eXlKTU1VWFhYerevbsaNGjgdJ6Tk5N18uRJrz7PK1euVHBwsO6///4Kt6tN57VVq1YKDQ11Ope5ubnau3ev41z27t1b2dnZOnDggGObbdu2qaSkxBEUvcXVYHT8+HFt2bJFzZs3r3SfpKQk+fj4lLoF5W1Onz6t8+fPO75va9N5tVq+fLm6d++uzp07V7ptTTy3lf2tuZbfv71799Y///lPp/B79T8Ct99++3UVAxdZu3atsdvtZtWqVeaHH34w48aNMwEBAU6z5r3Rc889Z/z9/c2OHTtMenq6Y7l06ZIxxpiUlBQzZ84cs3//fpOWlmY++ugj07p1a9O3b18PV141f/jDH8yOHTtMWlqa+eabb0xMTIwJCgoyWVlZxhhjnn32WdOyZUuzbds2s3//ftO7d2/Tu3dvD1dddcXFxaZly5Zm2rRpTu214bxevHjRHDp0yBw6dMhIMvPnzzeHDh1yvENr7ty5JiAgwHz00Ufm8OHD5sEHHzStWrUyP/30k+MYgwcPNl27djV79+41X3/9tbntttvMyJEjPTWkclU01itXrpgHHnjA3HLLLSYpKcnp5/jqO3h2795tFixYYJKSkkxqaqp55513TIsWLcwTTzzh4ZGVVtFYL168aP74xz+axMREk5aWZrZs2WK6detmbrvtNnP58mXHMbzlvBpT+fexMcbk5OSYJk2amKVLl5ba31vObWV/a4yp/PdvUVGRufPOO83AgQNNUlKS2bRpk2nRooWJj4+/rloIRy62ePFi07JlS9OwYUPTq1cvs2fPHk+XdMMklbmsXLnSGGPMyZMnTd++fU1gYKCx2+3m1ltvNS+88ILJycnxbOFV9Oijj5qwsDDTsGFDc/PNN5tHH33UpKSkOPp/+uknM378eHPTTTeZJk2amN/85jcmPT3dgxXfmC+++MJIMsnJyU7tteG8bt++vczv3dGjRxtjfn47/4wZM0xISIix2+1mwIABpf4dzp8/b0aOHGmaNWtm/Pz8zFNPPWUuXrzogdFUrKKxpqWllftzvH37dmOMMQcOHDDR0dHG39/fNGrUyHTo0MH813/9l1OgqCkqGuulS5fMwIEDTYsWLUyDBg1MZGSkGTt2bKn/pHrLeTWm8u9jY4x56623TOPGjU12dnap/b3l3Fb2t8aYa/v9++OPP5rY2FjTuHFjExQUZP7whz+YwsLC66rF9v8KAgAAgJhzBAAA4IRwBAAAYEE4AgAAsCAcAQAAWBCOAAAALAhHAAAAFoQjAAAAC8IRAACABeEIAADAgnAEAABgQTgC4BFXrlzxdAluUVfGCdQmhCMALnHx4kWNGjVKTZs2VVhYmBYsWKB+/fppypQpkqSoqCj9+c9/1hNPPCE/Pz+NGzdOkjRt2jS1bdtWTZo0UevWrTVjxgwVFhY6jjtr1ix16dJFK1asUMuWLdWsWTONHz9excXFeu211xQaGqrg4GD953/+5zXVaYzRrFmz1LJlS9ntdoWHh2vy5MmO/oKCAk2bNk0RERGy2+269dZbtXz5ckf/zp071atXL9ntdoWFhenFF19UUVGRo79fv36aOHGipkyZoqCgIA0aNEiSdOTIEcXGxqpZs2YKCQnR7373O507d67K/94Aqg/hCIBLxMXF6ZtvvtHHH3+szZs366uvvtLBgwedtnn99dfVuXNnHTp0SDNmzJAk+fr6atWqVfrhhx/03//93/r73/+uBQsWOO2Xmpqqzz//XJs2bdK7776r5cuX6/7779fp06e1c+dOvfrqq5o+fbr27t1baZ0ffvihFixYoLfeekvHjx/Xhg0b1LFjR0f/E088oXfffVeLFi3S0aNH9dZbb6lZs2aSpDNnzmjIkCHq2bOnvvvuOy1dulTLly/XX/7yF6fXePvtt9WwYUN98803WrZsmbKzs3Xvvfeqa9eu2r9/vzZt2qTMzEw98sgjVfq3BlDNDADcoNzcXNOgQQPzwQcfONqys7NNkyZNzPPPP2+MMSYyMtI89NBDlR5r3rx5pnv37o71mTNnmiZNmpjc3FxH26BBg0xUVJQpLi52tLVr184kJCRUevw33njDtG3b1ly5cqVUX3JyspFkNm/eXOa+L730kmnXrp0pKSlxtC1ZssQ0a9bMUcs999xjunbt6rTfn//8ZzNw4ECntlOnThlJJjk5udKaAbgXV44A3LB//etfKiwsVK9evRxt/v7+ateundN2PXr0KLXve++9pz59+ig0NFTNmjXT9OnTdfLkSadtoqKi5Ovr61gPCQnR7bffLh8fH6e2rKysSmt9+OGH9dNPP6l169YaO3as1q9f77gtlpSUpHr16umee+4pc9+jR4+qd+/estlsjrY+ffooLy9Pp0+fdrR1797dab/vvvtO27dvV7NmzRxL+/btJf18VQxAzUI4AuA2TZs2dVpPTEzUqFGjNGTIEG3cuFGHDh3Syy+/XGoSc4MGDZzWbTZbmW0lJSWV1hAREaHk5GS9+eabaty4scaPH6++ffuqsLBQjRs3ruLInP1ynHl5eRo6dKiSkpKcluPHj6tv374ueU0ArlPf0wUA8H6tW7dWgwYNtG/fPrVs2VKSlJOTo//5n/+p8I//7t27FRkZqZdfftnRduLEiWqvt3Hjxho6dKiGDh2qCRMmqH379vrnP/+pjh07qqSkRDt37lRMTEyp/Tp06KAPP/xQxhjH1aNvvvlGvr6+uuWWW8p9vW7duunDDz9UVFSU6tfn1y5Q03HlCMAN8/X11ejRo/XCCy9o+/bt+v777zVmzBj5+Pg43YL6pdtuu00nT57U2rVrlZqaqkWLFmn9+vXVWuuqVau0fPlyHTlyRP/617/0zjvvqHHjxoqMjFRUVJRGjx6tp59+Whs2bFBaWpp27Nih999/X5I0fvx4nTp1SpMmTdKxY8f00UcfaebMmYqLi3O6xfdLEyZM0IULFzRy5Ejt27dPqamp+uKLL/TUU0+puLi4WscL4PoRjgC4xPz589W7d2/9+te/VkxMjPr06aMOHTqoUaNG5e7zwAMPaOrUqZo4caK6dOmi3bt3O97FVl0CAgL097//XX369FGnTp20ZcsWffLJJ2revLkkaenSpfrtb3+r8ePHq3379ho7dqzy8/MlSTfffLM+++wzffvtt+rcubOeffZZjRkzRtOnT6/wNcPDw/XNN9+ouLhYAwcOVMeOHTVlyhQFBARUGKoAeIbNGGM8XQSA2ic/P18333yz3njjDY0ZM8bT5QDANePmNwCXOHTokI4dO6ZevXopJydHc+bMkSQ9+OCDHq4MAK4P13MBuMzVhzzGxMQoPz9fX331lYKCgtxaw+rVq53eMm9d7rjjDrfWAsA7cVsNQK1y8eJFZWZmltnXoEEDRUZGurkiAN6GcAQAAGDBbTUAAAALwhEAAIAF4QgAAMCCcAQAAGBBOAIAALAgHAEAAFgQjgAAACz+Lyjj2DqavF0DAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.histplot(combined_text.gram_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4a922b-df82-4600-b975-290c648c5e31",
   "metadata": {},
   "source": [
    "Defining a new metric to check the quality of text chunk. New metric: ratio_gram_to_words = Number of grammar mistakes in a text chunk/Number of words in the text chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fcf521c3-ad63-4828-9ed5-1a54f40d6726",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_text[\"ratio_gram_to_words\"] = combined_text[\"gram_score\"]/combined_text[\"text_length\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2452e3e0-9e7f-461d-902c-2236ea9b6bc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='ratio_gram_to_words', ylabel='Count'>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.histplot(combined_text.ratio_gram_to_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5dd0b90-b7a6-48d7-bec2-e04665863afb",
   "metadata": {},
   "source": [
    "#### We keep the text chunks that have grammar mistakes to words ratio less than 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9e17f022-085f-4457-bb79-850edb679e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_clean_text = combined_text[combined_text.ratio_gram_to_words < 0.15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7e5e5074-c594-4c7b-9cf7-d662810e95d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1289"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f2eaf3-7535-4960-a2e3-781557fd2cc8",
   "metadata": {},
   "source": [
    "### Generate embeddings for the text chunks in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9a4aa9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'all-MiniLM-L6-v2'  \n",
    "emb_model = SentenceTransformer(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cc6c0923",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = all_clean_text.text.apply(emb_model.encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "80977a71-4a99-4796-b27a-9282a21f7ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_clean_text = all_clean_text.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e9afc628",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_clean_text[\"embeddings\"] = X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cfb0aa8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_length</th>\n",
       "      <th>gram_score</th>\n",
       "      <th>ratio_gram_to_words</th>\n",
       "      <th>embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>goal the goal of the programmin g project is t...</td>\n",
       "      <td>300</td>\n",
       "      <td>28</td>\n",
       "      <td>0.093333</td>\n",
       "      <td>[-0.046663534, 0.0423988, -0.028370954, -0.053...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>project step 1 project definition submission m...</td>\n",
       "      <td>300</td>\n",
       "      <td>27</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>[-0.05021884, -0.018155381, -0.023710225, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>choose something related to health exercise fo...</td>\n",
       "      <td>300</td>\n",
       "      <td>16</td>\n",
       "      <td>0.053333</td>\n",
       "      <td>[0.00020982492, 0.07735407, -0.020393653, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>with important idea highlighted explore datase...</td>\n",
       "      <td>72</td>\n",
       "      <td>2</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>[0.051482484, 0.0065475046, -0.049179822, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>goal during the lecture w e covered many aspec...</td>\n",
       "      <td>300</td>\n",
       "      <td>27</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>[-0.007569024, 0.023703393, -0.037298728, -0.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  text_length  gram_score  \\\n",
       "0  goal the goal of the programmin g project is t...          300          28   \n",
       "1  project step 1 project definition submission m...          300          27   \n",
       "2  choose something related to health exercise fo...          300          16   \n",
       "3  with important idea highlighted explore datase...           72           2   \n",
       "4  goal during the lecture w e covered many aspec...          300          27   \n",
       "\n",
       "   ratio_gram_to_words                                         embeddings  \n",
       "0             0.093333  [-0.046663534, 0.0423988, -0.028370954, -0.053...  \n",
       "1             0.090000  [-0.05021884, -0.018155381, -0.023710225, -0.0...  \n",
       "2             0.053333  [0.00020982492, 0.07735407, -0.020393653, -0.0...  \n",
       "3             0.027778  [0.051482484, 0.0065475046, -0.049179822, -0.0...  \n",
       "4             0.090000  [-0.007569024, 0.023703393, -0.037298728, -0.0...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_clean_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "be424822-5cdf-422f-bff5-f910c4c02ad6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1289"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e6989095-4f61-4c78-b584-b05e289eb516",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_clean_text.to_csv(\"final_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df549d6d-b006-4498-bf38-52a6d1d62b0e",
   "metadata": {},
   "source": [
    "### We use DBSCAN Clustering to further filter out non-usable text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6703b92f-857a-4d3e-b320-6b21106aa77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "dbscan_obj = DBSCAN(eps=0.95, min_samples=4)\n",
    "dbscan_obj.fit(X.to_list())\n",
    "dbscan_clusters = dbscan_obj.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "803b6eb5-d404-498c-9811-a037d75d5cec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0    1179\n",
       "-1     100\n",
       " 1       5\n",
       " 2       5\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(dbscan_clusters).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b85ed9e-6aa3-434e-83a5-2acdd94d2214",
   "metadata": {},
   "source": [
    "#### Visually inspect the clusters to choose which to drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "600b8c23-c2de-42a6-a137-5c7090e95e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_clean_text[\"cluster\"] = dbscan_clusters\n",
    "temp = all_clean_text.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b67a10da-649c-435b-9e65-19345df0f9d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['with important idea highlighted explore dataset for determining it s coherence 2h a summary of observation about the dataset which could impact our future development write code for preprocessing dataset required to make data into proper format for 3h dataset in format z develop baseline system to have a simple system to compare to 4h baseline system question you can ask your question on brightspace in the project area of the forum',\n",
       " '061572 076155 09184 072823 059806 016884 059675 016543 089073 0060983 comfort 045849 091592 027884 067579 10385 013239 00029672 029708 029086 014638 0011615 036847 069142 061768 010487 040521 050348 0017701 048911 11228 017567 1019 080644 016709 057987 072542 062351 00096739 10891 036178 19589 1195 0038978 01175 047923 014341 0049582 028468 0013233 059639 036529 0053435 033613 03281 038041 024016 010248 022486 024354 046712 ralph 089005 028722 029375 020416 10173 040185 11642 10235 05229 066993 0093718 0275 059325 081409 044641 0091214 037466 03537 010802 073682 010244 02483 050697 020149 016329 047134 029646 04719 029636 03282 031125 020933 0057354 10292 0049036 024412 0017263 028363 023106 022451 0023439 092904 069422 090352 02107 020974 064822 12694 024252 1335 1869 03681 032056 07805 10423 027921 04636 090183 018945 099069 11887 01245 025086 031206 070986 0085944 036447 089217 00096013 099771 096641 13968 07243 016906 025744 0065666 033128 036096 062237 0084945 064592 066111 12237 043886 04805 0060063 045997 050485 063666 040003 057787 0014727 10246 009467 044955 10421 034727 035703 12056 021645 011689 25intent detection nlu sourcespectrum cepstrum mfccprobabilities on letter a b z blank for each time window deep learning architecture 28speech to text conversion asr can these bereplaced by a latent representation 32user identification wake word detection speech to text conversion asr intent detection nlu pretraining learning latent representation transfer using the representation in matching pretraining fine tuning transfer no fine tuning but transfer in finetuning the learning of the model would continue in the second step pretraining fine tuning transfer pretraining fine tuning transfer 34supervised training aillotarget prediction pretraining can beto learn a latent representationlanguage model for rescoringhello world hey lowworld helllowspeech to text conversion asr 35speech to text conversion asr selfsupervised approach hubert to learning a speech signal representation the same a bert try to generate a context richrepresentation of word hubert try to generate context',\n",
       " 'looking for in your report is clear answer student is able to express hisher idea clearly concise an swer student is able to express hisher idea in a few paragraph plus illustration plus comparative table when required good information gatheringanalysis student spend timeeffort for finding goodinteresting information in articlesblogsvideos and is able to analyze this information good argumentation student is able to express hisher ideaopinion using good justification source provided student cite hisher source article blog video image question you can ask your question in the discussion forum on brightspace',\n",
       " 'goal of the presentation introduction to intent detection intent modeling language input wordsentence representation system architecture intent classification slot filling evaluation call sarah call mom on speaker set the timer for 10 minute tell lucy i am going to the store send email to john doe about tomorrow and say too busy to come play the voice mail from mom what is a 20 percent tip on 68 what is 234 divided by 6 what time is it in tokyo when is labor day take a picture set an alarm for six hour from now turn off all alarm whats the weather like today turn on bluetooth where is the nearest hairdresser play music by brad meldhau play previous song check flight status of air canada 327 whats the nearest museumwhat isthe intent what do user want to do 7 call sarah call mom on speaker play music by brad meldhau play previous song turn off all alarm turn on bluetooth tell lucy i am going to the store send email to john doe about tomorrow and say too busy to come play the voice mail from mom set the timer for 10 minute set an alarm for six hour from now what is a 20 percent tip on 68 what is 234 divided by 6 call text email play voice mail set timer set alarmturn onoff calculatorplay music 9decide on a fixed set of intent intent detection intent classification call text message emailwho contactlist speaker onoff who contactlist message string who contactlist object string message string 10for each intent define the necessary optional parameter to perform the action call sarah call mom on speaker tell lucy i am going to the store send email to john doe about tomorrow and say too busy to come call text message emailwho',\n",
       " 'you will not be penalized for submitting audio only o make sure you are not only reading the content of the slide and that y our presentation provide s additional information to enrich our appreciation of your slide plus or minus project pitch o participation in the project pitch presentation 5 point if group doe not participate o in top 5 most selected project bonus 2 point or in top 10 bonus 1 point question you can ask your question in the project section on brightspace']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp.text[temp.cluster == -1].to_list()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "11788077-611d-4c5f-b961-c3f32f2b7b53",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['goal the goal of the programmin g project is to f urther explore a topic covered in the course or a topic related to what we s aw in the course you can work in group 2 student max each member of the group is expected to work about 25 hour on the project the project will allow you to gather hand on experience on a subject of interest within the area of virtual assistant the more specific goal of this step 1 project definition is to decide on your project define it purpose deliverable timeline and have it approved before starting your project submission deadline there are three milestone in the project each with it deadline the milestone are mentioned below and your focus now is on your project definition the project is worth 26 separated a below 1 project definition 10 deadline thursday march 14th midnight to obtain professor feedback on friday or friday march 15th to obtain professor feedback t he following monday submit a small report to explain your choice of topic programming environment project objective etc see detail in evaluation section the professor will review your document and provide feedback approval or modification you might be asked for clarificationsmodifications before you can proceed with your proje ct 2 project presentationdemoreport 80 deadline april 3rd midnight submit a 1012 minute presentation the presentation should show your goal methodology result and include a demonstration of your software 3 peer evaluation 10 during the class of april 4th please note the requirement for the project presentation and the peer review will be provided on brightspace later in march also note that 10 12 minute is an estimate i will give more pre cise time requirement later on depending on the number of group topic in ai virtual assistant',\n",
       " 'project step 1 project definition submission method your project definition should be submitted in brightspace it s a group submission so you need to first register your group project group and then submit instruction project definition decide on a topic that you wish to explore here are some suggestionsideas but your own idea is most welcome previous projec t from student in 20222023 will be presented in class march 7th to give you more idea your topic should be within the scope of virtual assistant and be based on an existing software environment andor involve some programming you must submit your project definition and have your project approved if you are unsure about your idea i encourage you to discus your project idea before submission by sending me an email o work on a specific international competition find two qa task dataset s develop a qa system and test it on those datasets find intent detection data ets develop an intent classification system using deep learning or other approach and test it find a structured data to text nlg dataset develop a nlg system using deep learning or any other method and test it o develop a speech recognition system exploring package such a speechrecognition package in python make it domain specific so it would recognize spoken sentence related to a particular domain eg restaurant astronomy o read an article on intent detectionqakbqa and implement their idea evaluate the reproducibility of their approach by redoing it add a little variation to test a new id ea o find software on github for intent detection or speech recognition or qa and understand testadapt it make sure to include something that is your idea o build a domain specific chatbot in dialogflow in rasa or other platform by domain specific you can',\n",
       " 'choose something related to health exercise food time management sport etc o work on wake word detection by developing a full classification system and testing various w akewords project realisation once your project definition is approved you will have to implement your idea a you defined it you will later present your goalmethodo logyresultsdemo during the class of april 4th more specific requirement will be provided for the content of your presentation evaluation project definition the report you submit will be m arked on 10 point o title of your project 1 point o short description of goal and limitation 4 point what will you achieve learn produce and what is your prior knowledge of this task what algorithmsapproaches will you be testingdeveloping what will be the final deliverable and by whom could it be used or what would be it contribution to the field what are the project boundary that you are setting to be able to achieve your project within 25 hour number of people in the group in other word what is included excluded to make your achievement realistic o description and justification of the s oftware platform programming involved dataset involved 1 point o activity table in which you list the different activity involved in your project and for each activity you provide a time estimate this should be considered a a plan and can of course change a bit a you do your project in your final report you will need to show your planned activity table and your achieved activity table including who didwhat in your group 4 point example of activity table total of hour should be 25number of student in group activity why time planned deliverable read article gather knowledge about x 3h summary of the article to be shared by group',\n",
       " 'goal during the lecture w e covered many aspect related to fulfillme nt and response generation this assignment is an exploration into various topic to practice your comparative analytical skill you are required to do 4 comparative study out of the 5 below 1 compare company developing alexa s skill or other va s skill 2 compare open source voice assistant 3 compare different qa datasets unstr uctured or kgqa datasets knowledge graph 4 compare different open source chatbot development platform 5 compare different data totext generation datasets submission deadline wednesday march 13th midnight 1159p m submission method in brightspace there is a link for assignment 3 to do your submission submit a short report in pdf form at describing your comparative study if you work in group of 2 make sure both student submit the same report and that both your name are on the top page topic in ai virtual assistant assignment 3 comparative study instruction comparative study 1 skill development virtual assistant are part of our daily life meaning that not only is there research going on to continue to enhance the platform but there is also a full ecosystem of vendor specialized in the development of skill for customer to add on their phone to do explore a few skill development company and choose 3 for your analysis describe each vendor what it doe what is it target audience develop a set of 5 criterion for your comparative study the criterion could be with respect to type of skill they suggest eg home automation game etc type of service offered eg full programming code adaptation etc make a comparative table according to yo ur set of criterion highlight a few aspect found in your study element you find int eresting advice to make etc comparative study',\n",
       " '2 open source voice assistant this article 7 best open source voice assistant describes some assistant not part of t he large private company the article date back to 2021 so perhaps some of the va s described do not exist anymore do not hesitate to find other article pointing to open source va outside of apple amazon google etc there is a desire to develo p virtual assistant that will have particular feature often privacy is a main concern and something promoted by the open source developer to do explore a few of these assistant choose 3 assistant for your analysis describe each voice assistant what it doe how i t can be used etc develop a set of 5 criterion for your comparative study the criterion could be with respect to the adopting community privacy issue etc make a comparative table according to your set of criterion highlight a few aspect found in your study element you find int eresting advice to make etc comparative study 3a qa datasets this article 10 question answering datasets to build robust chatbot system list different datasets for qa to do explore a few of these datasets choose 3 datasets for your analysis you can choose some that are not mentioned in the article a well describe each dataset what it contains which international evaluation if any it wa used for who wa it developed by etc show an example of it content develop a set of 5 criterion for your comparative study the criterion could be with respect to the size application etc make a comparative table according to your set of criterion highlight a few aspect found in your study element you find int eresting advice to make etc comparative study 3b knowledge graph qa datasets this article knowledge graph']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp.text[temp.cluster == 0].to_list()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e75d3d59-a95d-4235-89b6-0d0c083e7b3f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['finance apis offer user diverse service for account management and staying informed about market event they enable remote access to bank account for transaction and provide feature like stock market news trading platform and cryptocurrency market dataquery for news market moving price quote chart index analysis from investor and expert etcbinance future leaderboard api is an api for querying the leaderboard of the binance future exchange any question or issue please contact me on telegram devnullzer0httpstmedevnullzer0 or via devnullzer0protonmemailtodevnullzer0protonmeget stock index market quote and trend etf international exchange forex crypto related news and analytics in realtimean api that provides information about credit card signup bonus spend category redemption category reward benefit card image and moretradingview api for technical analysis with indicator ocillicators summary etcthis api is obtained directly from yahoo finance restful api with json result start nowlookup a bank information based on a routing number input choose either ach or wire transfer bank information support xml and json responsesfinancial data provided for developer to enter the world market instant access for realtime and historical data of stock forex crypto etf index and more read more in documentationhttpstwelvedatacomdocs and start herehttpstwelvedatacomapikeythis api provides complete data from investingcom including stock index commodity news and many moresimple reliable api for current and historical foreign exchange forex currency ratesmboum finance official api for stock option etf mutual fund sec data historical earnings technical indicator live quote news screener and more yahoo finance api for stock option etf mutual fund historical earnings technical indicator live quote and news moved to httpsrapidapicomspariorapimboumfinanceindex calculated by httpsalternativemecryptothis api help you to query for data which is obtained by professional provider who have direct and extensive access to stock quote future popular index forex bitcoin and cfds to create a financial community siteapplication such a tradingviewcomprovides currency exchange',\n",
       " 'rate based on the market and national central bank datathis forex api delivers live currency rate economic news and both market sentiment indicator and real trader sentiment make smarter forex decision and build powerful applicationsa simple api for cryptocurrency price 28k user realtime historical crypto market data coin exchange free paidaccess analytical historical data for all option and future contract in greek financial derivative exchange athexthe world s most popular api for getting coverage of stock data breaking news and current headline from the u and around the world which help you to create application like the wall street journalthis api help to query financial summary stock quote mover news etc to create a financial siteapplication the simplest and most effective way to receive stock etf forex technical indicator and cryptocurrency data constantly ranked a a leading api provider for ease of use accuracy and price see the documentationhttpswwwalphavantagecodocumentation and read morehttpsmediumcompatrickcollins58673stockapilandscape5c6e054ee631 on the market data industry start now for freehttpswwwalphavantagecosupportapikeyunlock the power of aidriven market intelligence with markettrendanalyst the latest offering from econotrend this cuttingedge tool provides unparalleled insight into consumer behavior and market trend empowering business to make informed decision and stay ahead of the curve dive deep into market dynamic and shape your strategy with confidencemarket data api for intraday 1minutes data endofday data option data crypto forex live price fundamental data trading signal and much more on various asset stock etf fund index forex cryptocurrencies etc on worldwide stock exchange u canada australia uk and europesharpe two long and short volatility signal with the different data used in our analysis visit our website to learn more finance apis allow endusers a variety of service option for their account a well a to stay current on event and news that impact their portfolio and financial security for',\n",
       " 'example a finance api could remotely connect them to their bank account to initiate deposit transfer or other transaction other financial apis include stock market news and trading platform cryptocurrency market and morea financial api provides a secure link from a consumer to the database and transactional server of the institution with which they do businessapplication programming interface or apis are the digital link between data provider and endusers in the financial sector security is essential a sensitive information is transferred in realtime so only the most robust protocol are utilized to protect the data transmitted on both endsbanks benefit from incorporating financial apis to appease existing customer with stateoftheart benefit and entice new clientele by offering bestinclass security measure and option for their consumer consumer enjoy accessing secure account information and initiating most transaction through their mobile device and computerslikewise current market rate for stock cryptocurrency and more are essential for trader of all kind even web developer that run financial advice site can benefit from financial apis by offering current information that brings visitor to their site and building brand loyaltyin the notsodistant past all banking business required a trip to the physical branch trading decision were based on newspaper statistic detailing what the market wa doing on any given day however this antiquated way of doing thing is not conducive to success in the evervolatile and dynamic financial marketfinance apis provide consumer convenience appease the needitnow appetite of today s society and allow for more informed decision based on the most accurate and current information available many time updated by the minutewith technology advancing by the minute developer need software and apis that stay a step ahead of those looking to exploit it finance apis need to utilize the most advanced protocol necessary to protect the sensitive',\n",
       " 'information sent every second therefore not only should consumer be able to depend on realtime transaction and market statistic but they should be confident that none of their data can be intercepted or corrupted during the flow from inquiry to endpointall finance apis are supported and made available in multiple developer programming language and sdks includingjust select your preference from any api endpoint pagesign up today for free on rapidapi to begin using finance apis',\n",
       " 'official apifind out how much your airbnb can earn based on shortterm rental historical data point travel apis are the behindthescenes code that allows for uptodate information at the user s fingertip so whether a person is researching hotel price looking up airline itinerary or figuring out the best place to see at their destination there is an api working a the middle man between the researcher s device and the vast amount of information stored in the source databasestravel apis fetch data from different server that house it such a airline hotel destination etc the api then arranges the information a requested into sends it back to the user s application with all of the requested parameter sorted to display to the endusera travel api work behind the scene and connects a user s application to the requisite database supplying a secure data transfer between endpoint apis of this sort are usually built using restjson or soapxmlon the business side of travel airline hotel travel booking agency car rental place and more benefit from the use of travel apis by gaining more customer due to offering convenience to a broader audiencetravelers benefit by being able to cut down on research time immensely furthermore they can research the best deal to save money learn exciting place to go when they arrive at their destination and even perform task like check upcoming weather report in their desired localetravel apis connect business with consumer interested in exploring the world around them gone are the day of the yellow page and price checking that could take day week or even longer in search of great deal consumer are also no longer at the mercy of travel agent pushing vacation that may not be in their client s best interesttoday traveler have all of']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp.text[temp.cluster == 1].to_list()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5174ba1b-3e1c-4a50-9a4d-3d85b11c0061",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['username password data is available in many form shape and format broadly data can be either structured or unstructured data thats properly organized with welldefined constraint and relationship among it different part can be considered a structured there no precise definition of structured data in the hazy boundary between structured and unstructured data some have identified semistructured data others have argued that all data ha some structure it just that some are more difficult to store or analyze the general viewpoint is that all data can add value to data mining and analytics technology that evolved for structured data have been adapted and new one invented to handle unstructured data a well unstructured data ha become increasingly important due to it volume velocity variety and value data thats highly organized such a in a database can be considered a structured data these often use a relational database management system rdbms such data ha schema that defines attribute and their type constraint on value and relation with other data table and attribute data must conform to this schema this make data easy to query using structured query language sql and thereby facilitates analysis data in spreadsheet may be considered structured unstructured data is not organized in this manner that is in rdbms consider an audio stream which ha a welldefined format otherwise it couldnt be decoded and played consider newspaper text a linguist would say there structure but from the perspective of business analyst this data is unstructured simply because it harder to analyze and obtain insight some say the term unstructured data is a misnomer if it truly lacking structure it useless to store it or analyze it it would be better to categorize data a fixed or variable structure repetitive or hierarchical textual or nontextual rich medium such a',\n",
       " 'image video or audio are unstructured social medium generates lot of unstructured content website host unstructured content that are commonly textual in nature information in document such a m word file or pdf file are also seen a unstructured machinegenerated content such a satellite image iot sensor data or cctv video feed are considered unstructured many of the same data source mentioned above could have attribute that make them semistructured for instance tweet facebook post blog article and news story published online often have number of like retweetsshares and comment including name of reader who did these email text may be unstructured but the header contains name of senderreceiver date and subject that give some structure iot data may be seen a semistructured when it in json or xml format data about data called metadata such a author name and publication date make data semistructured in fact rich semantic markup on webpage give them lot more structure that what html alone doe most unstructured data can be considered a semistructured because of metadata in a manufacturing plant operation log and report are unstructured text analysis to pick out frequent word or sentiment can help the plant manager make a maintenance plan or quickly understand the nature of a particular line koorong book in australia us text analysis to identify duplicate posting or suggest similar book this is an example of contentbased profiling in banking customer may often give feedback or complaint on social medium rather than via web form if bank wish to be customer centric they need to act on this unstructured data in fact this could apply to any industry that need to listen to it customer company can use chatbots with nlp capability to automate customer support function deep learning technique are being used to analyze image',\n",
       " 'and sound image can be automatically labelled mammogram can be analyzed for cancer the sound of a motor can inform in advance if it going to fail this is of importance in automobile and aviation sector an early myth wa that unstructured data cant be quantitatively analyzed perhaps true in the past but with recent advance in computer vision speech processing and natural language processing algorithm are able solve many complex problem in these domain some might believe that unstructured data replaces structured data in reality many company have not fully exploited the potential of structured data good old predictive model and analytical capability should continue to be used unstructured data will give access to new insight not otherwise available in structured data in conclusion both structured and unstructured data are valuable another myth say that all big data is unstructured data telecom company smart energy meter smartphones and car fitted with sensor are all generating big data thats structured some business just store the data with a vague idea of using them later in fact data and insight depreciate over time realtime dashboard are probably the best opportunity to act on data in a timely manner when collecting or acting on data tie it to a business vision big data technology such hadoop and nosql database have come about to address the need of storing and managing unstructured data data warehouse and data lake are place where big data is stored unstructured data is often used alongside structured data many technology cater for both hadoop enables distributed storage and computing on big data it a good engine for handling unstructured data though it a myth to think that unstructured data cant be stored or analyzed without hadoop nosql database are highly scalable and support flexible schema their storage is',\n",
       " 'distributed theyre nonrelational theyre good at storing multimedia social medium or textual data among the different type are document store column store keyvalue store and graph data store a mix of these is often used each suited to a particular data this approach is called polyglot persistence a cost of flash memory drop flash becomes a faster alternative to disk storage however file service such a data protection backup and search need to be in place for a minimalistic storage system try the open source minio alternative include ceph scality and cleversafe the general perception is that unstructured data is hard to analyze this is changing due to advance in machine learning model the simplest way to get started is perhaps to call apis that others have published for example cognitive apis from ibm such a watson tradeoff analytics can help in decision making geneea is an nlp api for speech recognition we can use att speech api google cloud vision api is useful for many imagespecific task in the 1960s business start using computer storing and managing data becomes important to them because memory is expensive there a need to store data efficiently for these reason database are invented ibms ims is an example these early database store only structured data the 1970s see the arrival of relational database in 1974 ibm invents sql a a language to query such database andy rehn vp of marketing for data base architect state that a much a 90 percent of the information business us is nonnumerical freeform data this is one of the earliest published report that quantifies the prevalence of unstructured data the early and mid1990s is when text mining start entering realworld application documentmanagement system emerge later rebranded a enterprise content management ecm system the world wide web also',\n",
       " 'start generating lot of unstructured data business intelligence bi that had grown up on structured data start mining text for useful insight a rule of thumb is that 80 of all data is unstructured or semistructured at best this 80 figure is mentioned in a merrilllynch report but it not due to primary research from a survey involving data warehousing and business intelligence search tdwi research find that structuredunstructured ratio is not 2080 a often claimed structured data is at 47 unstructured data at 31 and the rest being semistructured however the report recognizes that unstructured data is on the rise oasis approves unstructured information management architecture uima version 10 apache uima is an open source implementation of this standard v100 of this software is released in january 2014 in august 2019 v310 is released analysis of unstructured data is still new in some domain a case in point is the healthcare industry where doctor handwritten note image history formula and genetics have not been fully analyzed username password']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp.text[temp.cluster == 2].to_list()[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6acc02c6-fa0d-43bb-82a1-8792a8eb6a2d",
   "metadata": {},
   "source": [
    "#### Removing ouliers (cluster = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "17e70f93-9167-49af-9e74-4f1032c2418d",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_text = all_clean_text[all_clean_text.cluster != -1].drop(columns=\"cluster\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c90aa332-b5f1-4949-b771-9abfa5f066d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_length</th>\n",
       "      <th>gram_score</th>\n",
       "      <th>ratio_gram_to_words</th>\n",
       "      <th>embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>goal the goal of the programmin g project is t...</td>\n",
       "      <td>300</td>\n",
       "      <td>28</td>\n",
       "      <td>0.093333</td>\n",
       "      <td>[-0.046663534, 0.0423988, -0.028370954, -0.053...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>project step 1 project definition submission m...</td>\n",
       "      <td>300</td>\n",
       "      <td>27</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>[-0.05021884, -0.018155381, -0.023710225, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>choose something related to health exercise fo...</td>\n",
       "      <td>300</td>\n",
       "      <td>16</td>\n",
       "      <td>0.053333</td>\n",
       "      <td>[0.00020982492, 0.07735407, -0.020393653, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>goal during the lecture w e covered many aspec...</td>\n",
       "      <td>300</td>\n",
       "      <td>27</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>[-0.007569024, 0.023703393, -0.037298728, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2 open source voice assistant this article 7 b...</td>\n",
       "      <td>300</td>\n",
       "      <td>25</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>[-0.011002384, -0.027711425, 0.00037316183, -0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  text_length  gram_score  \\\n",
       "0  goal the goal of the programmin g project is t...          300          28   \n",
       "1  project step 1 project definition submission m...          300          27   \n",
       "2  choose something related to health exercise fo...          300          16   \n",
       "4  goal during the lecture w e covered many aspec...          300          27   \n",
       "5  2 open source voice assistant this article 7 b...          300          25   \n",
       "\n",
       "   ratio_gram_to_words                                         embeddings  \n",
       "0             0.093333  [-0.046663534, 0.0423988, -0.028370954, -0.053...  \n",
       "1             0.090000  [-0.05021884, -0.018155381, -0.023710225, -0.0...  \n",
       "2             0.053333  [0.00020982492, 0.07735407, -0.020393653, -0.0...  \n",
       "4             0.090000  [-0.007569024, 0.023703393, -0.037298728, -0.0...  \n",
       "5             0.083333  [-0.011002384, -0.027711425, 0.00037316183, -0...  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ac4943cf-ab58-450e-9524-3c408c048149",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1189"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtered_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3822bc-dc97-4512-9bd0-b8d893b70e21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "61b1dc17-8d41-48f9-b252-bb17284a6780",
   "metadata": {},
   "source": [
    "#### Questions to ask the QA system:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa4a786-0a62-474f-a19c-2833a20c5f1c",
   "metadata": {},
   "source": [
    "The format of the question should be: \"Question: {The Question}? (a) {option 1} (b) {option 2} ...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1d0b5aac-0e50-48a5-bfea-472bc54fdba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Questions = [\n",
    "    \"Question: How does a Virtual Assistant typically communicate with clients? (a) Via telepathic communication (b) Through physical mail (c) Using the internet and communication tools like email, chat, and video calls (d) Through carrier pigeons\",\n",
    "    \"Question: What is the primary role of a Virtual Assistant? (a) To work on-site at the client’s office (b) To provide administrative support remotely (c) To manage physical office operations (d) To handle in-person customer interactions\",\n",
    "    \"Question: What is viewed as problem of probabilistic inference? (a) Speech recognition (b) Speaking (c) Hearing (d) Utterance\",\n",
    "    \"Question: Which of the following statements about speaker verification is true? (A) Text-dependent speaker verification is more secure and offers better verification performance. (B) Text-independent speaker verification is more secure but does not perform as well. (C) Text-dependent speaker verification is more secure but does not perform as well. (D) Text-independent speaker verification is not as secure but offers better performance.\",\n",
    "    \"Question: Which of the following factors does not have an impact on the results of  Facial Recognition Systems ? (a) Background Setting (b) Illumination Variations (c) Occlusion (d) Expression Variations\",\n",
    "    \"Question: What metric is defined as the ratio of the number of correct instances that were misclassified by a system to the total predictions called? (a) False Reject Rate (b) False Negatives (c) False Alarm Rate (d) Balanced\",\n",
    "    \"Question: MFCCs are a set of __ coefficients calculated per frame. Fill the blank with the number of coefficients. (a) 39 features (b) 12 features (c) 26 features (d) 13 features\",\n",
    "    \"Question: In order to create MFCCs, what is the mathematical transformation applied on the frames of audio samples and in which scale is the frequency used in it? (a) Fourier Transformation, Mel Scale (b) Cosine Transformation, Hertz Scale (c) Fourier Transformation, Hertz Scale (d) Cosine Transformation, Mel Scale\",\n",
    "    \"Question: What is the dense representation of text in the form of numerical vectors known as ? (a) Word Embeddings (b) BERT (c) Latent Embeddings (d) Pre-trained Embeddings\",\n",
    "    \"Question: Which of the following is a valid pair of question types that are studied for QA assistants ? (a) Factoid and Opinion Questions (b) Empirical and False Questions (c) Structured and Semi-Structured Questions (d) Open and Closed Questions\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c701021d-1f67-42c9-a0e1-a4c32a5e700d",
   "metadata": {},
   "source": [
    "#### Define Missions for the Answer generation system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "33cbe82f-8036-4b92-828e-b6c2d3574288",
   "metadata": {},
   "outputs": [],
   "source": [
    "mcq_mission = 'You are a Question Answering system for a university course called \"Virtual Assistants.\" The course is about software-based agents that provide administrative, technical, or creative assistance to users remotely. You will be asked to solve multiple-choice questions. Your task is to pick the option that is most factually correct, regardless of whether the given context is directly relevant. At the start of your answer mention the Option selected.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6128be8f-d2d2-4798-92eb-89096d15cc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_mission = \"You are a Question Answering system for a university course called Virtual Assistants. The course is about software-based agents that provide administrative, technical, or creative assistance to users remotely. You will be provided some context that you can use to answer the question. If the context does not have the answer, generate one from your domain knowledge or retrieve it from the provided course information.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e3a6c2-96b2-41ea-933c-1cf57ea4bfc1",
   "metadata": {},
   "source": [
    "Adding some information about the course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "259319da-c684-47aa-8fcb-33e942adbf4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "course_info = \"This course is taught by professor Caroline Barrière at the University of Ottawa in the Winter term. The professor’s email is cbarrier@uottawa.ca. All communications about this course will be done through BrightSpace. The class is held every Thursday, from 2:30–5:30 pm, in the room CRX C307. There is no mandatory book for this course. This class will be held in-person, so attendance is highly recommended. The goal of this course is to understand the various components playing a role in Virtual Assistants (VA), to know about various AI approaches/technologies used in VAs, know about the numerous challenges faced when building VAs, Be knowledgeable of existing applications and open source software for VAs, and have hands-on experience building a simple VA or a module within a VA. There are 3 assignments in this course, worth 24% in total. In addition, there will be two student presentations worth 10% each. There will be a final project worth 26% and a final exam worth 30%.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "668cc0fc-83fa-47de-8131-5d23dbf41b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19be6bff-4643-40fa-b376-bff151f6f122",
   "metadata": {},
   "source": [
    "Load the generation model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "08d627be-ea9f-478b-920a-dd5d08196292",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gemma's activation function should be approximate GeLU and not exact GeLU.\n",
      "Changing the activation function to `gelu_pytorch_tanh`.if you want to use the legacy `gelu`, edit the `model.config` to set `hidden_activation=gelu`   instead of `hidden_act`. See https://github.com/huggingface/transformers/pull/29402 for more details.\n",
      "Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████| 2/2 [00:15<00:00,  7.64s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "pipe = pipeline(\"text-generation\", model=\"google/gemma-2b-it\", device_map=\"cuda\", token=access_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "779490fd-7cb4-4ed8-a824-c60aaef47564",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b-it\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19daa92d-68f3-4474-8aa3-55d7c34fc41b",
   "metadata": {},
   "source": [
    "Load the summarizer model. This model can summarize given text chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "15a22833-252b-4275-a259-b0fd7a809319",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer = pipeline(\"summarization\", model=\"Falconsai/text_summarization\", device_map=\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41fb4d8-deff-4763-a427-e81dbf6879f2",
   "metadata": {},
   "source": [
    "### MCQ Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a28ffc16-fae2-4765-9444-ec794026353b",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevance_threshold = 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "92591d33-fc8b-4225-a6d7-9808cacd1b1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================================================================================================\n",
      "\n",
      "\n",
      "Question:  How does a Virtual Assistant typically communicate with clients? \n",
      "Options:  ['Via telepathic communication ', 'Through physical mail ', 'Using the internet and communication tools like email, chat, and video calls ', 'Through carrier pigeons'] \n",
      "\n",
      "\n",
      "Context Extracted: virtual assistant va is a software agent that can perform a range of tasks or service for a user based on user input such a command or question including verbal one such technology often incorporate chatbot capability to simulate human speech and respond via synthesized voice in many case user can ask their virtual assistant question control home automation device and medium playback and manage other basic task such as a email todo list and calendar . virtual assistant can provide a wide variety of service such as provide information such as a weather fact from eg wikipedia or imdb set an alarm make todo list and shopping list play music from streaming service such a spotify and pandora play radio station read audiobooks play video tv show or movie on television streaming service . an automated online assistant produced a 30 decrease in the workload for a humanprovided call centre enhance the driving experience . lucy rutherford is studying psychology at queen s university belfast . she managed the study into intelligent personal assistant while interning at sensumtwo academic paper that informed this story came from our collaborator dr gary mckeown from the school of psychology across the road from u . here are some relevant story on how ai is integrated into emerging technology such a autonomous and semiautonomous vehicle . tapia mji adopted smartmedical s empath a vocal emotion recognition technology used in various business field such a mental health call center and entertainment with empathe . tappingia can understand human emotion through dialogue with user joy calm sorrow anger and vigor tapian responds to user based on their emotion expressing her feeling by the eye and voice . the global robotics market is expanding rapidly and communication robot like tapiia have been developed all over the world . jivo chat is a live chat tool that allows you to manage and interact with customer in realtime through different communication channel such as a your website telegram facebook and viber . if the potential client decides to start a conversation you or your agent will receive an immediate notification on their mobile or computer to answer this question . it is recommended for any customer service team that need a simple and inexpensive online chat solution . kpi s a well a lead generation and intercom supportimperson is one of the leading agency in enterprise chatbots that support text audio video ar and vr on all major messaging platform it fullservice creative studio deploys and host your bot and provides an advanced analytics dashboard including realtime insight to improve performance they also monitor your chatbot s performance and continuously customize it based on user behavior .\n",
      "\n",
      " System output: \n",
      "\n",
      "The correct answer is C: Using the internet and communication tools like email, chat, and video calls\n",
      "\n",
      "The passage describes how virtual assistants can communicate with clients through the internet and communication tools like email, chat, and video calls.\n",
      "Time Taken =  110.1456983089447  seconds\n",
      "\n",
      "\n",
      "\n",
      "======================================================================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 150, but your input_length is only 42. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=21)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Question:  What is the primary role of a Virtual Assistant? \n",
      "Options:  ['To work on-site at the client’s office ', 'To provide administrative support remotely ', 'To manage physical office operations ', 'To handle in-person customer interactions'] \n",
      "\n",
      "\n",
      "Context Extracted: virtual assistant va is a software agent that can perform a range of tasks or service for a user based on user input such a command or question including verbal one such technology often incorporate chatbot capability to simulate human speech and respond via synthesized voice in many case user can ask their virtual assistant question control home automation device and medium playback and manage other basic task such as a email todo list and calendar . virtual assistant can provide a wide variety of service such as provide information such as a weather fact from eg wikipedia or imdb set an alarm make todo list and shopping list play music from streaming service such a spotify and pandora play radio station read audiobooks play video tv show or movie on television streaming service . an automated online assistant produced a 30 decrease in the workload for a humanprovided call centre enhance the driving experience . amazoncom cto werner vogels announced alexa for business saying the technology ha the potential to make it easy to manage the office environmentcozza said this mean vpas could find their way into meeting room integrating with software such a outlook and powerpoint introduce speaker . in this video we explain how organization can use amazon lex to deploy the conversational interface of their choice for their business process and use case to achieve their goal and enhance customer satisfaction by exceeding expectation . ebiai ha a team of specialist who review every conversation your ai assistant ha with your customer by doing this they are constantly improving the performance of your assistant . proprofs chat is designed specifically for business looking for realtime sale and support solution for their website the software enables user to build custom chatbots that automate support convert lead and grow salesbuilding chatbot .\n",
      "\n",
      " System output: \n",
      "\n",
      "**B**: To provide administrative support remotely\n",
      "\n",
      "**Explanation:** The passage describes virtual assistants as software agents that provide administrative, technical, or creative assistance to users remotely.\n",
      "Time Taken =  75.30389714241028  seconds\n",
      "\n",
      "\n",
      "\n",
      "======================================================================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 150, but your input_length is only 92. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=46)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Question:  What is viewed as problem of probabilistic inference? \n",
      "Options:  ['Speech recognition ', 'Speaking ', 'Hearing ', 'Utterance'] \n",
      "\n",
      "\n",
      "Context Extracted: is lower than 1 it will support h2 diminishing the posterior probability of h1the role of the practitioner and the judgejury are clearly separated thus decomposing the information in the case . the judge or jury can give their prior odds in a totally subjective way without the need to explicitly compute the probabilies . this is a process that we intrinsically perform a human in fact think about assigning prior odds to rain or not rain next time you leave home in order to decide if you take your umbrella with you or not onin bayesian decision theory decision are made according to the probability of the different hypothesis in the light of all the observed information ie the proposition variable h . p left h1 left right ei right and p right h2 left rightei left to represent those probability in their simplest form are mutually exclusive and exhaustive . now a problem arises the value of pleft h1, left right, and right, can not be computed by the judge or jury because judge and jury can dtw a well known application ha been automatic speech recognition to cope with different speaking speed in general it is a method that allows a computer to find an optimal match between two given sequence eg time series with certain restriction that is the sequence are warped nonlinearly to match each other this sequence alignment method is often used in the context of hidden markov model neural network emerged a acoustic modeling approach in asr in the late 1980s neural network have been used in many aspect of speech recognition such a digit recognition ha already reached a rate of 996 li 2008 . the same can not be said of phone recognition for which the best rate are still under 80 . research team continue developing phone recognizers in order to enhance their performance . ourlanguage and music are important way that we communicate with each other talking out loud and playing a guitar mayfor the most part we talk because we want to communicate with others our friend parent teacher even pet ourin this article we will show what our brain do when we listen to someone talking to u most particularly we will frontier frontier office . stress pattern listener tend to stick to what they know and their native language may influence how they perceive speech in another language if fleur listens to hermione teaching her the spell . stress contrast do not exist in french so she may not recognize the stress contrast for what it is 3 but do not worry she can still learn to recognize it . a device called a cochlear implant figure 2 can bring back some hearing for these listener a surgeon implant a wire with tiny electrode into part of the inner ear called our study of 18 tt and three human voice identifies clear trend in top performing voice but also echo prior work in voice design 34 36 46 49 . the pattern which emerged in our analysis raises several insight which we believe can inform future work on how hci researcher and practitioner might select a voice for a given application . our analysis allows u to make broad generalization that certain voice performed better than other voice for reading the article we chose . in spoken language analysis an utterance is a continuous piece of speech by one person before or after which there is silence on the part of the person in the case of oral language it is generally but not always bounded by silence do they exist in written language only their representation do they can be represented and delineated in many way in oralspoken language . the paralinguistic feature which are aspect of speech such a facial expression gesture and posture prosodic feature include stress intonation . the computer can rearrange the bit in any number of different way to create entirely new word and sentence 71 a any good actor can demonstrate a sentence can be read in different way depending on the meaning of the text the person speaking and the emotion it wish to express in linguistics this idea is known a prosody and is one of the most difficult problem for voice synthesizer source73today s research in tt .\n",
      "\n",
      " System output: \n",
      "\n",
      "The correct answer is [C]: Hearing\n",
      "\n",
      "The context describes the challenges of probabilistic inference in the context of hearing.\n",
      "Time Taken =  96.86207747459412  seconds\n",
      "\n",
      "\n",
      "\n",
      "======================================================================================================================================================\n",
      "\n",
      "\n",
      "Question:  Which of the following statements about speaker verification is true? \n",
      "Options:  ['Text-dependent speaker verification is more secure and offers better verification performance. ', 'Text-independent speaker verification is more secure but does not perform as well. ', 'Text-dependent speaker verification is more secure but does not perform as well. ', 'Text-independent speaker verification is not as secure but offers better performance.'] \n",
      "\n",
      "\n",
      "Context Extracted: automatic speaker veri cation2 outline introduction speaker identi bation v speech recognition speech recognition recognitionwordsspeaker identity4 speaker identifier . determine the speaker identity selection between a set of known voices the user doe not claim an identity closed set . assume that speaker is not among the speaker known to the system eg prompted phrase password phrase xed phrase knowing the text can improve system performance prompting may reduce risk of imposter . speaker recognition is the task of identifying a speaker using their voice recognition is classified into two part speaker identification and speaker verification . speaker identification is the process of determining which voice in a group of known voice best match the speaker speaker verification is computationally le complex than speaker identification system because they require a comparison between only one or two model whereas speaker identification requires comparison of one model to n speaker modelsspeaker verification method are divided into textdependent and textindependent method the speaker verification system ha prior knowledge about the text to be spoken speaker recognition is the task of identifying a speaker using their voice recognition is classified into two part speaker identification and speaker verification . speaker identification is the process of determining which voice in a group of known voice best match the speaker speaker verification is computationally le complex than speaker identification system because they require a comparison between only one or two model whereas speaker identification requires comparison of one model to n speaker modelsspeaker verification method are divided into textdependent and textindependent method the speaker verification system ha prior knowledge about the text to be spoken a system output score between 0 and 1 or between 5000 and 1000000 will be higher than the differentpeople score . some comparison between speech material coming from the same person may yield a lower score than some other comparison between different people and viceversa this is normal in reallife system every system make error however it is more true in forensic sience considerint the typically unfavourable condition of the speech thus although this behaviour is to be avoided perfection doe not exist . speaker recognition is the task of identifying a speaker using their voice recognition is classified into two part speaker identification and speaker verification . speaker identification is the process of determining which voice in a group of known voice best match the speaker speaker verification is computationally le complex than speaker identification system because they require a comparison between only one or two model whereas speaker identification requires comparison of one model to n speaker modelsspeaker verification method are divided into textdependent and textindependent method the speaker verification system ha prior knowledge about the text to be spoken a system output score between 0 and 1 or between 5000 and 1000000 will be higher than the differentpeople score . some comparison between speech material coming from the same person may yield a lower score than some other comparison between different people and viceversa this is normal in reallife system every system make error however it is more true in forensic sience considerint the typically unfavourable condition of the speech thus although this behaviour is to be avoided perfection doe not exist . speaker recognition is the task of identifying a speaker using their voice recognition is classified into two part speaker identification and speaker verification . speaker identification is the process of determining which voice in a group of known voice best match the speaker speaker verification is computationally le complex than speaker identification system because they require a comparison between only one or two model whereas speaker identification requires comparison of one model to n speaker modelsspeaker verification method are divided into textdependent and textindependent method the speaker verification system ha prior knowledge about the text to be spoken a system output score between 0 and 1 or between 5000 and 1000000 will be higher than the differentpeople score . some comparison between speech material coming from the same person may yield a lower score than some other comparison between different people and viceversa this is normal in reallife system every system make error however it is more true in forensic sience considerint the typically unfavourable condition of the speech thus although this behaviour is to be avoided perfection doe not exist . speaker recognition is the task of identifying a speaker using their voice recognition is classified into two part speaker identification and speaker verification . speaker identification is the process of determining which voice in a group of known voice best match the speaker speaker verification is computationally le complex than speaker identification system because they require a comparison between only one or two model whereas speaker identification requires comparison of one model to n speaker modelsspeaker verification method are divided into textdependent and textindependent method the speaker verification system ha prior knowledge about the text to be spoken a system output score between 0 and 1 or between 5000 and 1000000 will be higher than the differentpeople score . some comparison between speech material coming from the same person may yield a lower score than some other comparison between different people and viceversa this is normal in reallife system every system make error however it is more true in forensic sience considerint the typically unfavourable condition of the speech thus although this behaviour is to be avoided perfection doe not exist .\n",
      "\n",
      " System output: \n",
      "\n",
      "The correct answer is C: Text-dependent speaker verification is more secure but does not perform as well.\n",
      "Time Taken =  113.63239979743958  seconds\n",
      "\n",
      "\n",
      "\n",
      "======================================================================================================================================================\n",
      "\n",
      "\n",
      "Question:  Which of the following factors does not have an impact on the results of  Facial Recognition Systems ? \n",
      "Options:  ['Background Setting ', 'Illumination Variations ', 'Occlusion ', 'Expression Variations'] \n",
      "\n",
      "\n",
      "Context Extracted: facial recognition is the most natural mean of biometric identification the face recognition system doe not require any contact with the person the 1200 million electronic passports in circulation in 2021 provide a huge opportunity to implement face recognition at international border . the algorithm are getting extremely accurate with artificial intelligence according to a 2018 nist study the system developer have made massive gain in facial recognition accuracy . emotional ai can also be used to track eye movement by using infrared eyetracking camera or webcam to map pupil movement or dilation a well a gaze time . a person s respiration and heart rate can be detected contactlessly using cameras daniel mcduff what s made this possible is the investment in camera technology .\n",
      "\n",
      " System output: \n",
      "\n",
      "**Answer:** C. Occlusion\n",
      "\n",
      "**Explanation:** Occlusion is the absence of light, which can prevent the camera from capturing an image of the face. This can lead to decreased accuracy of the facial recognition system.\n",
      "Time Taken =  71.40299987792969  seconds\n",
      "\n",
      "\n",
      "\n",
      "======================================================================================================================================================\n",
      "\n",
      "\n",
      "Question:  What metric is defined as the ratio of the number of correct instances that were misclassified by a system to the total predictions called? \n",
      "Options:  ['False Reject Rate ', 'False Negatives ', 'False Alarm Rate ', 'Balanced'] \n",
      "\n",
      "\n",
      "Context Extracted: 40 63 50 55 60 47 70 44 80 36 90 36 10 figure 145 interpolated data point from fig 143 given curve . we can compare two system or approach by comparing their curve clearly curve that are higher in precision across all recall value are preferred . system that are more geared towards recall will be higher at higher level of recall to the right a second way to evaluate ranked retrieval is mean average precision mapmean average precision . precision and recall are then de ned a precision jrj jtjrecall juj1412 unfortunately these metric don t adequately measure the performance of a system thatranks the document it return . we need a metric that prefers the one that rank the relevant document higher we need to adapt Precision and recall to capture how well an . system doe at putting relevant document . higher in the ranking rank judgment precision rank recall rank 1 r 10 11 2 n 50 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 150, but your input_length is only 46. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=23)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " System output: \n",
      "\n",
      "A: False Reject Rate\n",
      "\n",
      "Explanation: False reject rate is the ratio of the number of false negatives to the total number of negatives. It tells us how well a system is at correctly rejecting false positives.\n",
      "Time Taken =  75.21703886985779  seconds\n",
      "\n",
      "\n",
      "\n",
      "======================================================================================================================================================\n",
      "\n",
      "\n",
      "Question:  MFCCs are a set of __ coefficients calculated per frame. Fill the blank with the number of coefficients. \n",
      "Options:  ['39 features ', '12 features ', '26 features ', '13 features'] \n",
      "\n",
      "\n",
      "Context Extracted: learning and deep learning method current achievement analysis and remaining challenge information 13 no 6 268 httpsdoiorg103390info13060268 subscribe to receive issue release notification and newsletter from mdpi journal .\n",
      "\n",
      " System output: \n",
      "\n",
      "A: 39 features\n",
      "\n",
      "The context describes MFCCs as a set of 39 features calculated per frame.\n",
      "Time Taken =  39.25857400894165  seconds\n",
      "\n",
      "\n",
      "\n",
      "======================================================================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 150, but your input_length is only 108. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=54)\n",
      "Your max_length is set to 150, but your input_length is only 108. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=54)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Question:  In order to create MFCCs, what is the mathematical transformation applied on the frames of audio samples and in which scale is the frequency used in it? \n",
      "Options:  ['Fourier Transformation, Mel Scale ', 'Cosine Transformation, Hertz Scale ', 'Fourier Transformation, Hertz Scale ', 'Cosine Transformation, Mel Scale'] \n",
      "\n",
      "\n",
      "Context Extracted: the inverse fourier transform is applied to the output on the right to characterize f1 f2 and f3 detail laterin addition there is duality in the fourier transformation convolution and multiplication can be interchanged asso if a function is hard to model we may model it in another domain on the other hand if the manipulation is hard we can perform the corresponding duality transformation above to see whether it may be easier to solve conceptually we are just switching tool whenever it is easierto extract audio feature we timit is one popular corpus that contains utterance from 630 north american speakersthe audio clip will be divided into frame a phone will occupy multiple frame with such a corpus we can learn how to perform the first half of the diagram is the audio for the fricative consonant sh it is clearly different from the vowel after it however for machine learning ml we need a denser representation so we can identify them easier engineer love the frequency domain we apply the fourier transform to convert time domain information fourier analysis relies on jas the imaginary unit euler s formula ejqcosqjsinq 165 a brief reminder for those student who have already studied signal processing the dft is de ned . low frequency and le resolution at high frequency figure 167 show a sample bank of triangular lters that implement this idea that can be multiplied by the spectrum to get a mel spectrum m1m2mmmel spectrum077000051amplitudefrequency hz8k figure 168 . the basic architecture for asr is the encoderdecoder implemented with either rnns or transformer exactly the same architecture introduced for mt in chap ter 13 generally we start from the log mel cosine similarity metric between two vector vandwthus can be computed a Cosine 65 tfidf w eighing term in the vector 11 . fig 68 shows a visualization 50010001500200025003000500digitalcherryinformationdimension 1 pie dimension 2 computer figure 68 . fourier analysis relies on jas the imaginary unit euler s formula ejqcosqjsinq 165 a brief reminder for those student who have already studied signal processing the dft is de ned . cosine similarity metric between two vector vandwthus can be computed a Cosine 65 tfidf w eighing term in the vector 11 . fig 68 shows a visualization 50010001500200025003000500digitalcherryinformationdimension 1 pie dimension 2 computer figure 68 .\n",
      "\n",
      " System output: \n",
      "\n",
      "**A**: Fourier Transformation, Mel Scale\n",
      "\n",
      "The mathematical transformation applied on the frames of audio samples is the Fourier Transformation. The frequency scale is used in the Mel Scale, which is a logarithmic scale of frequency.\n",
      "Time Taken =  104.35689282417297  seconds\n",
      "\n",
      "\n",
      "\n",
      "======================================================================================================================================================\n",
      "\n",
      "\n",
      "Question:  What is the dense representation of text in the form of numerical vectors known as ? \n",
      "Options:  ['Word Embeddings ', 'BERT ', 'Latent Embeddings ', 'Pre-trained Embeddings'] \n",
      "\n",
      "\n",
      "Context Extracted: a visualization 24 chapter 6 v ector semantics and embeddings method rohde et al 2006 probably the most common visualization method how ever is to project the 100 dimension of a word down into 2 dimension fig 61 showed one such visualization a doe fig 616 using a projection method called tsne van der maaten and hinton 2008 610 semantic property . the data is coming from immediately nearby word when vector are computed from long context window the highest co the word embedding appears to have described the mapping from the space of sparse count vector to the latent space of svd dense vector in lsa . the word thus originally meant mapping from one space to another it ha metonymically shifted to mean the resulting dense vector . it is in this sense that we currently use the word by the next decade bengio et al 2003 . the word embedding appears to have described the mapping from the space of sparse count vector to the latent space of svd dense vector in lsa . the word thus originally meant mapping from one space to another it ha metonymically shifted to mean the resulting dense vector . it is in this sense that we currently use the word by the next decade bengio et al 2003 . a visualization 24 chapter 6 v ector semantics and embeddings method rohde et al 2006 probably the most common visualization method how ever is to project the 100 dimension of a word down into 2 dimension fig 61 showed one such visualization a doe fig 616 using a projection method called tsne van der maaten and hinton 2008 610 semantic property . the data is coming from immediately nearby word when vector are computed from long context window the highest co bert is the greatest search engine ever able to find the answer to any question we pose itin part 1 of this post notebook i ll explain what it really mean to apply bert to qa and illustrate the detailspart 2 contains example code we ll be downloading a model that s already finetuned for question answering . bert performs well on text that wasn t in the squad datasetlinkscontentsby chris mccormick . Variational autoencoders learn to represent the input just in a compressed form called the latent space or the bottleneck . a variational auto Encoder would construct latent attribute in the following manner . the decoder reconstructing the input can be very easily understood via statistical expression . v 20 chapter 6 v ector semantics and embeddings 1wcaardvark zebrazebraaardvarkapricotapricotvv12v target wordscontext noisewords 1d figure 613 . the algorithm store two embeddeds for each word . wand the contextnoise embeddedings c 682 learning skipgram embedderings the learning algorithm . for each of these wcpostraining instance we ll create knegative sample each consist v 20 chapter 6 v ector semantics and embeddings 1wcaardvark zebrazebraaardvarkapricotapricotvv12v target wordscontext noisewords 1d figure 613 . the algorithm store two embeddeds for each word . wand the contextnoise embeddedings c 682 learning skipgram embedderings the learning algorithm . for each of these wcpostraining instance we ll create knegative sample each consist fig 1012 show the idea x compositeembeddingsword positiontransformer blockjanet1will2back3janetwillbackthebillthe4bill5 . position embeddings are both of size 1d so their sum is also 1d . ap proach is to choose a static function that map integer input to realvalued vector in a way that capture the inherent relation ship among the position that is it capture the fact that position 4 in an input is more closely related to\n",
      "\n",
      " System output: \n",
      "\n",
      "**A**: Word Embeddings\n",
      "\n",
      "Therefore, the correct answer is **A**: Word Embeddings.\n",
      "Time Taken =  99.18942499160767  seconds\n",
      "\n",
      "\n",
      "\n",
      "======================================================================================================================================================\n",
      "\n",
      "\n",
      "Question:  Which of the following is a valid pair of question types that are studied for QA assistants ? \n",
      "Options:  ['Factoid and Opinion Questions ', 'Empirical and False Questions ', 'Structured and Semi-Structured Questions ', 'Open and Closed Questions'] \n",
      "\n",
      "\n",
      "Context Extracted: question answering qa is a computer science discipline within the field of information retrieval and natural language processing nlp that is concerned with building system that automatically answer question that are posed by human in a natural language overview . questionanswering system can query a structured database of knowledge or information usually a knowledge base . system can pull answer from an unstructured collection of natural language document a local collection of reference text internal organization document and web page compiled newswire report . the dataset contains 127000 question with answer collected from 8000 conversations get the dataset herehotpotqa is a dataset which contains 113k wikipediabased questionanswer pair with four key feature these are question that require finding and reasoning over multiple supporting document to answer the question are diverse and not constrained to any preexisting knowledge base or knowledge schema sentencelevel supporting fact required for reasoning . questionanswering system typically included a question classifier module that determined the type of question . the retriever is aimed at retrieving relevant document related to a given question while the reader is used to infer the answer from the retrieved document system such a gpt3 t5 and bart use an endtoend architecture in which a transformerbased architecture store largescale textual data . the system takes a natural language question a an input rather than a set of keywords for example when is the national day of china it then transforms this input sentence into a query in it logical form . the system will have to identify the correct one in order to give a sensible answer assigning a question type to the question is a crucial task the entire answer extraction process relies on finding the correct question type and hence the correct answer type keyword extraction is the first step in identifying the input question type . knowledge graph can be subdivided into the ontology commonly referred to the schema or tboxo c r c and the set of fact instance or socalled abox e b r. e.b c. l 14 22 knowledge graph question answering kgqa aim at retrieving the correct answer or respectively answer denoted as of a given natural language question .\n",
      "\n",
      " System output: \n",
      "\n",
      "A: Factoid and Opinion Questions\n",
      "\n",
      "The context describes that question answering system typically included a question classifier module that determined the type of question. Therefore, the correct answer is A: Factoid and Opinion Questions.\n",
      "Time Taken =  92.72909092903137  seconds\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for Q in Questions:\n",
    "    t1 = time.time()\n",
    "    extracted_info = extract_QA(Q)\n",
    "    q_emb = emb_model.encode(extracted_info[\"Question\"].lower())\n",
    "    option_embs = [emb_model.encode(o.lower()) for o in extracted_info[\"Options\"]]\n",
    "    retrieved_text, score = get_relevant_text(q_emb, all_clean_text, 2, return_score=True)\n",
    "    relevant_text = filter_context(retrieved_text, score, relevance_threshold)\n",
    "    \n",
    "    print(\"==\"*75)\n",
    "    #print(f\"\\nQuestion: Distance from retrieved context: {score[0]}\")\n",
    "    #if score[0] < relevance_threshold:\n",
    "    #relevant_text.append(retrieved_text.text.to_list())\n",
    "        \n",
    "    i = 0\n",
    "    for o in option_embs:\n",
    "        i+=1\n",
    "        retrieved_text, score = get_relevant_text(o, all_clean_text, 2, return_score=True)\n",
    "        rel_text = filter_context(retrieved_text, score, relevance_threshold)        \n",
    "        #print(f\"Option {i} : Distance from retrieved context: {score[0]}\")\n",
    "        relevant_text.extend(rel_text)\n",
    "\n",
    "            \n",
    "    #relevant_text = [element for sublist in relevant_text for element in sublist]\n",
    "    relevant_text = summarizer(relevant_text, max_length=150, min_length=30, do_sample=False)\n",
    "    relevant_text = \" \".join(d['summary_text'] for d in relevant_text)\n",
    "\n",
    "    txt = \"\"\n",
    "    for i in range(len(extracted_info['Options'])):\n",
    "        txt = f\"{txt} [{chr(ord('A')+i)}]: {extracted_info['Options'][i]}\"\n",
    "        \n",
    "    input_text = f\"Context: {relevant_text} \\n [Question]: {extracted_info['Question']} [Options]: {txt}\"\n",
    "    \n",
    "    print(\"\\n\\nQuestion: \", extracted_info['Question'], \"\\nOptions: \", extracted_info[\"Options\"], \"\\n\\n\")\n",
    "    print(f\"Context Extracted: {relevant_text}\")\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": mcq_mission,\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"ok\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": input_text\n",
    "        },\n",
    "        \n",
    "    ]\n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    outputs = pipe(prompt, max_new_tokens=128, do_sample=True, temperature=0.3, top_k=30, top_p=0.98, return_full_text=False)\n",
    "    print(\"\\n System output: \\n\")\n",
    "    print(outputs[0][\"generated_text\"])\n",
    "    print(\"Time Taken = \", time.time()-t1, \" seconds\")\n",
    "    print(\"\\n\\n\")\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88dde04-6f32-4eac-a758-1b59f187fdac",
   "metadata": {},
   "source": [
    "### Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b9936453-0e47-4619-bd84-be29154f5a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "subjective_questions = [\n",
    "    \"What are transformers?\",\n",
    "    \"How is this course graded?\",\n",
    "    \"Who is the professor for this course?\",\n",
    "    \"Compare text-dependent and text-independent speaker verification\",\n",
    "    \"What are the components of a Virtual Assistant?\",\n",
    "    \"How can we convert audio data into a learnable form?\",\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c00466da-869d-4329-95d1-0fab40550529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================================================================================================\n",
      "Question:  What are transformers? \n",
      "\n",
      "\n",
      "Context Extracted: transformer block each of which is a multilayer network that map sequence of input vector x1xnto sequence of output vector z1znof the same length these block are made by combin ing simple linear layer feedforward network and selfattention layer . transformer selfattention allows a network to directly extract and use information from arbitrarily large context we ll start by describing how self attention work . a transformer is a deep learning architecture developed by google and based on the multihead attention mechanism proposed in a 2017 paper attention is all you need text is converted to numerical representation called token . each token is converted into a vector via looking up from a word embedding table at each layer . the token is then contextualized within the scope of the context window with other unmasked tokens .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\logging\\__init__.py\", line 1160, in emit\n",
      "    msg = self.format(record)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\logging\\__init__.py\", line 999, in format\n",
      "    return fmt.format(record)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\logging\\__init__.py\", line 703, in format\n",
      "    record.message = record.getMessage()\n",
      "                     ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\logging\\__init__.py\", line 392, in getMessage\n",
      "    msg = msg % self.args\n",
      "          ~~~~^~~~~~~~~~~\n",
      "TypeError: not all arguments converted during string formatting\n",
      "Call stack:\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\base_events.py\", line 639, in run_forever\n",
      "    self._run_once()\n",
      "  File \"C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\base_events.py\", line 1985, in _run_once\n",
      "    handle._run()\n",
      "  File \"C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 359, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 446, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_17344\\1934137815.py\", line 48, in <module>\n",
      "    outputs = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.2, top_k=20, top_p=0.95, return_full_text=False)\n",
      "  File \"C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\pipelines\\text_generation.py\", line 240, in __call__\n",
      "    return super().__call__(text_inputs, **kwargs)\n",
      "  File \"C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\pipelines\\base.py\", line 1167, in __call__\n",
      "    logger.warning_once(\n",
      "  File \"C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\utils\\logging.py\", line 329, in warning_once\n",
      "    self.warning(*args, **kwargs)\n",
      "Message: 'You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset'\n",
      "Arguments: (<class 'UserWarning'>,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Taken =  152.73659944534302  seconds\n",
      "\n",
      "System Output:\n",
      "\n",
      "Sure, here's a summary of the context about transformers:\n",
      "\n",
      "- Transformers are a deep learning architecture developed by Google.\n",
      "- They are based on the multihead attention mechanism.\n",
      "- Self-attention allows a network to directly extract and use information from arbitrarily large context.\n",
      "- Each token is converted into a vector via looking up from a word embedding table at each layer.\n",
      "- The token is then contextualized within the scope of the context window with other unmasked tokens.\n",
      "====================================================================================================\n",
      "\n",
      "\n",
      "\n",
      "======================================================================================================================================================\n",
      "Question:  How is this course graded? \n",
      "\n",
      "\n",
      "Context Extracted: your report will be evaluated according to the following content o title page name title of project 1 point o intro 2 point describe the link between paraphrase detection and intent detection o dataset 10 point describe what part of the dataset you will use give your method for changing from graded evaluation to binary if you do a binary classifier give example of paraphrase and non paraphrase in a table discus the subjectivity of the class and the agree ment between the 5 judge\n",
      "Time Taken =  354.8396966457367  seconds\n",
      "\n",
      "System Output:\n",
      "\n",
      "**Title Page**\n",
      "\n",
      "**Virtual Assistants: A Comprehensive Course on Software-Based Agents**\n",
      "\n",
      "**Introduction**\n",
      "\n",
      "This course provides a comprehensive overview of software-based agents (VAs) and their applications. Students will learn about the various components and technologies involved in creating VAs, including natural language processing (NLP), machine learning (ML), and computer vision. The course also covers the challenges and opportunities associated with building and deploying VAs.\n",
      "\n",
      "**Dataset**\n",
      "\n",
      "The dataset for this course will consist of graded and ungraded evaluations of VAs. The graded evaluations will be used to determine the student's understanding of the course material. The ungraded evaluations will be used for training and testing the various algorithms and techniques used in the course.\n",
      "\n",
      "**Grading**\n",
      "\n",
      "The course will be graded based on the following criteria:\n",
      "\n",
      "* **Paraphrase Detection (24%)**: Students will be assessed on their ability to identify and distinguish between paraphrase and non-paraphrase sentences.\n",
      "* **Intent Detection (24%)**: Students will be assessed on their ability to identify the intent of a given text using NLP and ML techniques.\n",
      "* **Dataset (10%)**: Students will be assessed on their understanding of the dataset and their ability to identify relevant features\n",
      "====================================================================================================\n",
      "\n",
      "\n",
      "\n",
      "======================================================================================================================================================\n",
      "Question:  Who is the professor for this course? \n",
      "\n",
      "\n",
      "Context Extracted: \n",
      "Time Taken =  48.551029443740845  seconds\n",
      "\n",
      "System Output:\n",
      "\n",
      "Answer: Professor Caroline Barrière\n",
      "\n",
      "Context does not mention the professor's name, so I cannot answer this question from the provided context.\n",
      "====================================================================================================\n",
      "\n",
      "\n",
      "\n",
      "======================================================================================================================================================\n",
      "Question:  Compare text-dependent and text-independent speaker verification \n",
      "\n",
      "\n",
      "Context Extracted: speaker recognition is the task of identifying a speaker using their voice recognition is classified into two part speaker identification and speaker verification . speaker identification is the process of determining which voice in a group of known voice best match the speaker speaker verification is computationally le complex than speaker identification system because they require a comparison between only one or two model whereas speaker identification requires comparison of one model to n speaker modelsspeaker verification method are divided into textdependent and textindependent mfccs12 models hidden markov model are predominantly used type of hmm depends on application fixed phrase word or phrase models prompted phrase utterance model bulit from phone models gaussian mixture model gmm13 textindependent speaker veri cation the speaker model is built using speaker adaptation relatively small amount of speech map adaptation from the imposter model14 decision the decision is a 2class hypothesis test h0 the speaker is automatic speaker veri cation2 outline introduction speaker identi bation v speech recognition speech recognition recognitionwordsspeaker identity4 speaker identifier . determine the speaker identity selection between a set of known voices the user doe not claim an identity closed set . assume that speaker is not among the speaker known to the system eg prompted phrase password phrase xed phrase knowing the text can improve system performance prompting may reduce risk of jfa modeling defines two distinct space the speaker space defined by the eigenvoice matrix and the channel space represented by the channel matrix . the channel factor estimated using factor analysis a feature extractor that defines only a single space instead of two separate space in this new space a given speech recording is represented by a new vector called total factor a it contains the speaker and channel variability simultaneously speaker recognition based on the ivector framework . this browser is no longer supportedupgrade to microsoft edge to take advantage of the latest feature security update and technical supportspeaker recognition can help determine who is speaking in an audio clip . the service can verify and identify speaker by their unique voice characteristic by using voice biometryyou can then crosscheck audio voice sample against this profile to see if it matches any profile in the group speaker identificationimportantmicrosoft limit access to speaker recognition you can apply for access through the azure \n",
      "Time Taken =  254.26394295692444  seconds\n",
      "\n",
      "System Output:\n",
      "\n",
      "Sure, here's the comparison between text-dependent and text-independent speaker verification:\n",
      "\n",
      "**Text-Dependent Speaker Verification:**\n",
      "\n",
      "* The system uses a pre-recorded database of speech samples from known speakers.\n",
      "* The system compares the speech sample to the database and returns the identity of the speaker with the highest similarity score.\n",
      "* This method is simple to implement but can be less accurate than text-independent verification.\n",
      "\n",
      "**Text-Independent Speaker Verification:**\n",
      "\n",
      "* The system uses a machine learning algorithm to train a model to distinguish between speakers.\n",
      "* The model can be trained on a large dataset of speech samples from known speakers.\n",
      "* This method is more accurate than text-dependent verification but can be more complex to implement.\n",
      "====================================================================================================\n",
      "\n",
      "\n",
      "\n",
      "======================================================================================================================================================\n",
      "Question:  What are the components of a Virtual Assistant? \n",
      "\n",
      "\n",
      "Context Extracted: virtual assistant va is a software agent that can perform a range of tasks or service for a user based on user input such a command or question including verbal one such technology often incorporate chatbot capability to simulate human speech and respond via synthesized voice in many case user can ask their virtual assistant question control home automation device and medium playback and manage other basic task such as a email todo list and calendar . virtual assistant can provide a wide variety of service such as provide information such as a weather fact from eg wikipedia or imdb set an alarm make todo list and shopping list play music from streaming service such a spotify and pandora play radio station read audiobooks play video tv show or movie on television streaming service . an automated online assistant produced a 30 decrease in the workload for a humanprovided call centre enhance the driving smart advisor are another kind of consumer-based ai programming that can be regarded a soupedup virtual assistant subjectoriented rather than taskoriented smart advisor can function with littletono human supervision . smart advisor is also integrated on dedicated platform that abound today smart speaker top the list of this kind alexa hey siri ok google and hey cortana depending on the device . virtual assistant tech throwdown the term virtual assistant used to refer only to contract worker who perform administrative task from home but in the digital now the name evolved to define application and software that can do task and complete service for a user based on voice command and question you can think of them a your executive assistant or secretary in digital form virtual assistant also go by other similar term like ai assistant digital assistant . virtual assistant and their designer are controversial for spurring job insecurity and the ai they propose are still human in the way that they would be impossible without the microwork of million of human workersprivacy concern are raised by the fact that voice command are available to the provider of virtual assistant in unencrypted form and can thus be processed in an unexpected manner additionally to the linguistic content of recorded speech . google provides the action on google and dialogflow platform for developer\n",
      "Time Taken =  120.21190738677979  seconds\n",
      "\n",
      "System Output:\n",
      "\n",
      "Sure, here are the components of a virtual assistant:\n",
      "\n",
      "* User interface\n",
      "* Natural language processing (NLP)\n",
      "* Machine learning (ML)\n",
      "* Speech recognition\n",
      "* Natural language generation (NLG)\n",
      "* Knowledge base\n",
      "* Task management\n",
      "* Communication channel\n",
      "====================================================================================================\n",
      "\n",
      "\n",
      "\n",
      "======================================================================================================================================================\n",
      "Question:  How can we convert audio data into a learnable form? \n",
      "\n",
      "\n",
      "Context Extracted: the next question will be what is x the audio signal contains noisy information we are going to extract feature from the audio waveform . x will be the feature vector an expert can perform speech recognition from a spectrogram . we need an even denser representation this will force u to learn the core information but not the noisehowever our discussion so far doe not paint a full picture we speak what we hearin speech recognition knowing how we speak is this article will look at research and model architecture that have been written and developed to do just that using deep learning . the concatenative approach speech from a large database is used to generate new audible speech in a case where a different style of speech is needed a new database of audio voice is used this limit the scalability of this approach . the pronunciation model is fed into an acoustic model which basically defines how doe a given token sound . the data would be x which is the sequence of frame of audio feature from x1 to xt typically these feature are something that signal processing expert have defined such a the frequency component of the audio waveform that are capturedeach of these different component in this pipeline us a different statistical modelonce we have this kind of model built we can sound is transmitted a wave how do we turn sound wave into number let s use this sound clip of me saying hello sound wave are onedimensional at every moment in time they have a single value based on the height of the wave . for speech recognition a sampling rate of 16khz 16000 . author of this paper are from google they introduce a method known a probability density distillation which train a parallel feedforward network from a trained wavenet the method ha been built by marrying the best feature of inverse autoregressive flow iafs and wavenet . the author propose additional loss function for guiding the student in generating high quality audio streams .\n",
      "Time Taken =  396.557005405426  seconds\n",
      "\n",
      "System Output:\n",
      "\n",
      "Sure, here's how we can convert audio data into a learnable form:\n",
      "\n",
      "1. **Feature extraction**: Extract relevant features from the audio signal that are not affected by noise. These features could include frequency, time, and spectral information.\n",
      "\n",
      "\n",
      "2. **Data augmentation**: Generate new audio samples by applying various transformations to existing samples, such as adding noise, changing pitch, or altering tempo. This can help to increase the size and diversity of the training data.\n",
      "\n",
      "\n",
      "3. **Feature selection**: Select a subset of the features that are most important for the task at hand. This can be done using various techniques, such as feature importance scores or cross-validation.\n",
      "\n",
      "\n",
      "4. **Model training**: Train a machine learning model on the augmented and selected features. There are many different machine learning algorithms that can be used for speech recognition, including support vector machines, neural networks, and deep learning models.\n",
      "\n",
      "\n",
      "5. **Evaluation**: Evaluate the performance of the trained model on a held-out test set. This will give you an estimate of how well the model will perform on unseen data.\n",
      "\n",
      "\n",
      "6. **Iteration**: If the model does not perform as well as desired, iterate on the training process by adjusting the model parameters, trying different feature extraction methods, or\n",
      "====================================================================================================\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for Q in subjective_questions:\n",
    "    print(\"==\"*75)\n",
    "    t1 = time.time()\n",
    "    q_emb = emb_model.encode(Q)\n",
    "    relevant_text = []\n",
    "    # closest_cluster =  get_closest_cluster(q_emb, cluster_dict)[0]\n",
    "    retrieved_text, score = get_relevant_text(q_emb, all_clean_text, 5, return_score=True)\n",
    "    retrieved_text = retrieved_text.text.to_list()\n",
    "    for i in range(len(score)):\n",
    "        #print(score[i])\n",
    "        if score[i] < 0.6:\n",
    "            relevant_text.append(retrieved_text[i])\n",
    "            \n",
    "    #relevant_text = [element for sublist in relevant_text for element in sublist]\n",
    "    relevant_text = summarizer(relevant_text, max_length=100, min_length=30, do_sample=False)\n",
    "    relevant_text = \" \".join(d['summary_text'] for d in relevant_text)\n",
    "        \n",
    "    input_text = f\"Context: {relevant_text} \\n [Question]: {Q}\"\n",
    "    \n",
    "    print(\"Question: \", Q, \"\\n\\n\")\n",
    "    print(f\"Context Extracted: {relevant_text}\")\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": qa_mission,\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"ok\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"This is some information about this course: {course_info} \\n you may use this to answer questions.\"\n",
    "        },\n",
    "        \n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"ok\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": input_text\n",
    "        },\n",
    "    ]\n",
    "    \n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    # outputs = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.3, top_k=30, top_p=0.98, return_full_text=False)\n",
    "    outputs = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.2, top_k=20, top_p=0.95, return_full_text=False)\n",
    "    print(\"Time Taken = \", time.time()-t1, \" seconds\")\n",
    "    print(\"\\nSystem Output:\\n\")\n",
    "    print(outputs[0][\"generated_text\"])\n",
    "    print(\"==\"*50)\n",
    "    print(\"\\n\\n\")\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "754f268e-8263-4800-9f67-e66e2b127344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# **1. Preprocessing (Illustrative)**\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")  # Load a simple spaCy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f806a3e4-bf32-4fa6-a4d1-187b06c44cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# **2. TF-IDF**\n",
    "vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "13c0df4a-d6fd-4bc1-aa36-08b25c871d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total mins: 0.917\n"
     ]
    }
   ],
   "source": [
    "corpus = [txts for txts in filtered_text['text']]\n",
    "start = datetime.now()\n",
    "processed_corpus = [preprocess(doc) for doc in corpus]\n",
    "end = datetime.now()\n",
    "total = (end-start).seconds/60\n",
    "print(f'Total mins: {total:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "48e0d069-b7ef-45e3-b37c-f1ab6539f84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_index, vectorizer = build_index_with_faiss(corpus)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "62e1d398-4a8c-4374-a71d-c1e10bf65984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 150, but your input_length is only 121. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=60)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  Question: How does a Virtual Assistant typically communicate with clients? (a) Via telepathic communication (b) Through physical mail (c) Using the internet and communication tools like email, chat, and video calls (d) Through carrier pigeons \n",
      "\n",
      "\n",
      "Context Extracted: virtual assistant used to refer only to contract worker who perform administrative task from home but in the digital now the name evolved to define application and software that can do task and complete service for a user based on voice command and question you can think of them a your executive assistant or secretary in digital form virtual assistant also go by other similar term like ai assistant digital assistant and intelligent personal assistant . virtual assistant and their designer are controversial for spurring job insecurity and the ai they propose are still human in the way that they would be impossible without the microwork of million of human workersprivacy concern are raised by the fact that voice command are available to the provider of virtual assistant in unencrypted form and can thus be processed in an unauthorized or unexpected manner additionally to the linguistic content of recorded speech . google provides the action on google and dialogflow platform for developer to create action for google assistant apple provides sirikit communication is a process where all the aspect affect one another it means that communication is holistic and that the entire process creates a system in which all the element all the message work together for the common good . a conversational ui cui definition may look likean interface based on a holistic system of functional adaptive and meaningful message exchange . jivo chat is a live chat tool that allows you to manage and interact with customer in realtime through different communication channel such as a your website telegram facebook and viber . if the potential client decides to start a conversation you or your agent will receive an immediate notification on their mobile or computer to answer this question . it is recommended for any customer service team that need a simple and inexpensive online chat solution . zendesk is one of the best live chat tool for a wide range of business whether small or large it can be a good fit for 247 online service portal that need a broad knowledge base and faq search also the free plan may be sufficient for smaller website that only need to have one chat at a timeolark is an excellent alternative for those business that want to attract potential customer increase sale and offer support olark s advanced plan offer everything you need for optimal interaction .\n",
      "Time Taken =  105.27752661705017  seconds\n",
      "\n",
      "System Output:\n",
      "\n",
      "Answer: (c) Using the internet and communication tools like email, chat, and video calls\n",
      "\n",
      "Explanation: Virtual assistants typically communicate with clients through the internet and communication tools like email, chat, and video calls. These tools allow the virtual assistant to interact with the client in real-time and provide them with the necessary assistance.\n",
      "====================================================================================================\n",
      "\n",
      "\n",
      "\n",
      "======================================================================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 150, but your input_length is only 121. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=60)\n",
      "Your max_length is set to 150, but your input_length is only 141. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=70)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  Question: What is the primary role of a Virtual Assistant? (a) To work on-site at the client’s office (b) To provide administrative support remotely (c) To manage physical office operations (d) To handle in-person customer interactions \n",
      "\n",
      "\n",
      "Context Extracted: virtual assistant used to refer only to contract worker who perform administrative task from home but in the digital now the name evolved to define application and software that can do task and complete service for a user based on voice command and question you can think of them a your executive assistant or secretary in digital form virtual assistant also go by other similar term like ai assistant digital assistant and intelligent personal assistant . virtual assistant and their designer are controversial for spurring job insecurity and the ai they propose are still human in the way that they would be impossible without the microwork of million of human workersprivacy concern are raised by the fact that voice command are available to the provider of virtual assistant in unencrypted form and can thus be processed in an unauthorized or unexpected manner additionally to the linguistic content of recorded speech . google provides the action on google and dialogflow platform for developer to create action for google assistant apple provides sirikit customer feedback data for further analysishere are some way chatbot sentimental analysis can enhance user experience chatbot record the entire customer conversation . chatbots can modify their response so that they re aligned with the customer s emotion angry customer are routed to the right team to start a conversation to deliver personalized and effective customer support 3 use of ai in contact centers . chat she creates contextual insightful and conversational content for business audience across a broad range of industry and category like customer service customer experience cx chatbots and more serving a the lead content strategist snigdha patel is a customer experience researcher author and blogger .\n",
      "Time Taken =  71.61895155906677  seconds\n",
      "\n",
      "System Output:\n",
      "\n",
      "The correct answer is (b) To provide administrative support remotely. \n",
      "\n",
      "Virtual assistants provide administrative support remotely by handling tasks such as scheduling appointments, managing calendars, responding to emails, and providing customer support.\n",
      "====================================================================================================\n",
      "\n",
      "\n",
      "\n",
      "======================================================================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 150, but your input_length is only 129. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  Question: What is viewed as problem of probabilistic inference? (a) Speech recognition (b) Speaking (c) Hearing (d) Utterance \n",
      "\n",
      "\n",
      "Context Extracted: most recent book on speech recognition is automatic speech recognition a deep learning approach publisher springer written by microsoft researcher d yu and l deng . published near the end of 2014 with highly mathematically oriented technical detail on how deep learning method are derived and implemented in modern speech recognition system based on dnns . speech processing is the study of speech signal and the processing method of signal the signal are usually processed in a digital representation so speech processing can be regarded a special case of digital signal processing applied to speech signal aspect of speech processing includes the acquisition manipulation storage transfer . speech recognition speech synthesis speaker diarization speech enhancement speaker recognition etc history early attempts at speech processing and recognition were primarily focused on understanding a handful of simple phonetic element such a vowel in 1952 three researcher at bell lab stephen balashek r hermionegood hearing is important for understanding prosody after all it would be hard to link prosodic pattern to their function if you could not hear the pattern in first place this is the case for listener who hear very little or are completely deaf . fortunately a device called a cochlear implant figure 2 can bring back some hearing for these listener . paul grice 1989 came up with four maxims necessary in order to have a collegial conversation in which utterance are understood maxim of quantity provide the right amount of information needed for that conversation maxim of quality provide information that is true maxim of relation . bakhtins theory of the speech genre is archived 20140730 at the wayback machine . in spoken language analysis an utterance is a continuous piece of speech by one person before or after which there is silence on the part of the person in the case of oral language it is generally but not always bounded by silence do they exist in written language only their representation do they can be represented and delineated in many way in oralspoken language . the paralinguistic feature which are aspect of speech such a facial expression gesture and posture prosodic feature include stress intonation .\n",
      "Time Taken =  82.27619624137878  seconds\n",
      "\n",
      "System Output:\n",
      "\n",
      "The correct answer is (a) Speech recognition.\n",
      "\n",
      "The context describes the problem of probabilistic inference, which is a problem in which a system must be able to make accurate predictions about an unknown variable based on a set of observed data.\n",
      "====================================================================================================\n",
      "\n",
      "\n",
      "\n",
      "======================================================================================================================================================\n",
      "Question:  Question: Which of the following statements about speaker verification is true? (A) Text-dependent speaker verification is more secure and offers better verification performance. (B) Text-independent speaker verification is more secure but does not perform as well. (C) Text-dependent speaker verification is more secure but does not perform as well. (D) Text-independent speaker verification is not as secure but offers better performance. \n",
      "\n",
      "\n",
      "Context Extracted: speaker recognition is the task of identifying a speaker using their voice recognition is classified into two part speaker identification and speaker verification . speaker identification is the process of determining which voice in a group of known voice best match the speaker speaker verification is computationally le complex than speaker identification system because they require a comparison between only one or two model whereas speaker identification requires comparison of one model to n speaker modelsspeaker verification method are divided into textdependent and textindependent method the speaker verification system ha prior knowledge about the text to be spoken this browser is no longer supportedupgrade to microsoft edge to take advantage of the latest feature security update and technical supportspeaker recognition can help determine who is speaking in an audio clip the service can verify and identify speaker by their unique voice characteristic by using voice biometryyou can then crosscheck audio voice sample against this profile to see if it matches any profile in the group speaker identificationimportantmicrosoft limit access to speaker recognition you can apply for access through the azure ai service speaker recognition limited access review . speaker recognition is the task of identifying a speaker using their voice recognition is classified into two part speaker identification and speaker verification . speaker identification is the process of determining which voice in a group of known voice best match the speaker speaker verification is computationally le complex than speaker identification system because they require a comparison between only one or two model whereas speaker identification requires comparison of one model to n speaker modelsspeaker verification method are divided into textdependent and textindependent method the speaker verification system ha prior knowledge about the text to be spoken this browser is no longer supportedupgrade to microsoft edge to take advantage of the latest feature security update and technical supportspeaker recognition can help determine who is speaking in an audio clip the service can verify and identify speaker by their unique voice characteristic by using voice biometryyou can then crosscheck audio voice sample against this profile to see if it matches any profile in the group speaker identificationimportantmicrosoft limit access to speaker recognition you can apply for access through the azure ai service speaker recognition limited access review . speaker recognition is the task of identifying a speaker using their voice recognition is classified into two part speaker identification and speaker verification . speaker identification is the process of determining which voice in a group of known voice best match the speaker speaker verification is computationally le complex than speaker identification system because they require a comparison between only one or two model whereas speaker identification requires comparison of one model to n speaker modelsspeaker verification method are divided into textdependent and textindependent method the speaker verification system ha prior knowledge about the text to be spoken this browser is no longer supportedupgrade to microsoft edge to take advantage of the latest feature security update and technical supportspeaker recognition can help determine who is speaking in an audio clip the service can verify and identify speaker by their unique voice characteristic by using voice biometryyou can then crosscheck audio voice sample against this profile to see if it matches any profile in the group speaker identificationimportantmicrosoft limit access to speaker recognition you can apply for access through the azure ai service speaker recognition limited access review . speaker recognition is the task of identifying a speaker using their voice recognition is classified into two part speaker identification and speaker verification . speaker identification is the process of determining which voice in a group of known voice best match the speaker speaker verification is computationally le complex than speaker identification system because they require a comparison between only one or two model whereas speaker identification requires comparison of one model to n speaker modelsspeaker verification method are divided into textdependent and textindependent method the speaker verification system ha prior knowledge about the text to be spoken this browser is no longer supportedupgrade to microsoft edge to take advantage of the latest feature security update and technical supportspeaker recognition can help determine who is speaking in an audio clip the service can verify and identify speaker by their unique voice characteristic by using voice biometryyou can then crosscheck audio voice sample against this profile to see if it matches any profile in the group speaker identificationimportantmicrosoft limit access to speaker recognition you can apply for access through the azure ai service speaker recognition limited access review . speaker recognition is the task of identifying a speaker using their voice recognition is classified into two part speaker identification and speaker verification . speaker identification is the process of determining which voice in a group of known voice best match the speaker speaker verification is computationally le complex than speaker identification system because they require a comparison between only one or two model whereas speaker identification requires comparison of one model to n speaker modelsspeaker verification method are divided into textdependent and textindependent method the speaker verification system ha prior knowledge about the text to be spoken this browser is no longer supportedupgrade to microsoft edge to take advantage of the latest feature security update and technical supportspeaker recognition can help determine who is speaking in an audio clip the service can verify and identify speaker by their unique voice characteristic by using voice biometryyou can then crosscheck audio voice sample against this profile to see if it matches any profile in the group speaker identificationimportantmicrosoft limit access to speaker recognition you can apply for access through the azure ai service speaker recognition limited access review .\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 10.92 GiB is allocated by PyTorch, and 22.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[61], line 60\u001b[0m\n\u001b[0;32m     58\u001b[0m prompt \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mapply_chat_template(messages, tokenize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, add_generation_prompt\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# outputs = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.3, top_k=30, top_p=0.98, return_full_text=False)\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mpipe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.95\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_full_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTime Taken = \u001b[39m\u001b[38;5;124m\"\u001b[39m, time\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;241m-\u001b[39mt1, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mSystem Output:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\pipelines\\text_generation.py:240\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__call__\u001b[1;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[0;32m    238\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(chats, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    239\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtext_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\pipelines\\base.py:1206\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[1;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1198\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[0;32m   1199\u001b[0m         \u001b[38;5;28miter\u001b[39m(\n\u001b[0;32m   1200\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1203\u001b[0m         )\n\u001b[0;32m   1204\u001b[0m     )\n\u001b[0;32m   1205\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1206\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\pipelines\\base.py:1213\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[1;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[0;32m   1211\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[0;32m   1212\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpreprocess_params)\n\u001b[1;32m-> 1213\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1214\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(model_outputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_params)\n\u001b[0;32m   1215\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\pipelines\\base.py:1112\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[1;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[0;32m   1110\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[0;32m   1111\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m-> 1112\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1113\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m   1114\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\pipelines\\text_generation.py:327\u001b[0m, in \u001b[0;36mTextGenerationPipeline._forward\u001b[1;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[0;32m    324\u001b[0m         generate_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin_length\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m prefix_length\n\u001b[0;32m    326\u001b[0m \u001b[38;5;66;03m# BS x SL\u001b[39;00m\n\u001b[1;32m--> 327\u001b[0m generated_sequence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    328\u001b[0m out_b \u001b[38;5;241m=\u001b[39m generated_sequence\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\generation\\utils.py:1575\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[0;32m   1567\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[0;32m   1568\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   1569\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[0;32m   1570\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   1571\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   1572\u001b[0m     )\n\u001b[0;32m   1574\u001b[0m     \u001b[38;5;66;03m# 13. run sample\u001b[39;00m\n\u001b[1;32m-> 1575\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1576\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1577\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1578\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1579\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1580\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1581\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1582\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1583\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_logits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1584\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1585\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1587\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1588\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1590\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH:\n\u001b[0;32m   1591\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[0;32m   1592\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[0;32m   1593\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   1594\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1599\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[0;32m   1600\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\generation\\utils.py:2697\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[1;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, output_logits, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[0;32m   2694\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[0;32m   2696\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[1;32m-> 2697\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2698\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2699\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   2700\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2701\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2702\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2704\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[0;32m   2705\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\gemma\\modeling_gemma.py:1105\u001b[0m, in \u001b[0;36mGemmaForCausalLM.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[0;32m   1102\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m   1104\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[1;32m-> 1105\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1106\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1107\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1108\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1109\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1110\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1111\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1112\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1113\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1114\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1115\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1116\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1118\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1119\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(hidden_states)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\gemma\\modeling_gemma.py:923\u001b[0m, in \u001b[0;36mGemmaModel.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[0;32m    912\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    913\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    914\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    920\u001b[0m         cache_position,\n\u001b[0;32m    921\u001b[0m     )\n\u001b[0;32m    922\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 923\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    925\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    926\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    927\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    928\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    929\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    930\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    931\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    933\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    935\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\gemma\\modeling_gemma.py:643\u001b[0m, in \u001b[0;36mGemmaDecoderLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, **kwargs)\u001b[0m\n\u001b[0;32m    640\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[0;32m    642\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[1;32m--> 643\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    644\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    645\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    646\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    647\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    648\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    649\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    650\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    651\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    652\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    653\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[0;32m    655\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\gemma\\modeling_gemma.py:571\u001b[0m, in \u001b[0;36mGemmaSdpaAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position)\u001b[0m\n\u001b[0;32m    569\u001b[0m     query_states \u001b[38;5;241m=\u001b[39m query_states\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[0;32m    570\u001b[0m     key_states \u001b[38;5;241m=\u001b[39m key_states\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[1;32m--> 571\u001b[0m     value_states \u001b[38;5;241m=\u001b[39m \u001b[43mvalue_states\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontiguous\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    573\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mscaled_dot_product_attention(\n\u001b[0;32m    574\u001b[0m     query_states,\n\u001b[0;32m    575\u001b[0m     key_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    578\u001b[0m     dropout_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention_dropout \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0.0\u001b[39m,\n\u001b[0;32m    579\u001b[0m )\n\u001b[0;32m    581\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 10.92 GiB is allocated by PyTorch, and 22.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "for Q in Questions:\n",
    "    t1 = time.time()\n",
    "    extracted_info = extract_QA(Q)\n",
    "    q_emb = emb_model.encode(extracted_info[\"Question\"].lower())\n",
    "    option_embs = [emb_model.encode(o.lower()) for o in extracted_info[\"Options\"]]\n",
    "    retrieved_text = answer_question(extracted_info[\"Question\"].lower(), inverted_index, corpus, vectorizer, n=2)\n",
    "    score = [cosine(emb_model.encode(x), q_emb) for x in retrieved_text]\n",
    "    relevant_text = filter_context(retrieved_text, score, relevance_threshold, baseline=True)\n",
    "    \n",
    "    print(\"==\"*75)\n",
    "    #print(f\"\\nQuestion: Distance from retrieved context: {score[0]}\")\n",
    "    #if score[0] < relevance_threshold:\n",
    "    #relevant_text.append(retrieved_text.text.to_list())\n",
    "        \n",
    "    i = 0\n",
    "    for o in option_embs:\n",
    "        retrieved_text = answer_question(extracted_info[\"Options\"][i].lower(), inverted_index, corpus, vectorizer, n=2)\n",
    "        score = [cosine(emb_model.encode(x), o) for x in retrieved_text]\n",
    "        rel_text = filter_context(retrieved_text, score, relevance_threshold, baseline=True)        \n",
    "        #print(f\"Option {i} : Distance from retrieved context: {score[0]}\")\n",
    "        i+=1\n",
    "        relevant_text.extend(rel_text)\n",
    "\n",
    "            \n",
    "    #relevant_text = [element for sublist in relevant_text for element in sublist]\n",
    "    relevant_text = summarizer(relevant_text, max_length=150, min_length=30, do_sample=False)\n",
    "    relevant_text = \" \".join(d['summary_text'] for d in relevant_text)\n",
    "        \n",
    "    input_text = f\"Context: {relevant_text} \\n [Question]: {Q}\"\n",
    "    \n",
    "    print(\"Question: \", Q, \"\\n\\n\")\n",
    "    print(f\"Context Extracted: {relevant_text}\")\n",
    "    \n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": qa_mission,\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"ok\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"This is some information about this course: {course_info} \\n you may use this to answer questions.\"\n",
    "        },\n",
    "        \n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"ok\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": input_text\n",
    "        },\n",
    "    ]\n",
    "    \n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    # outputs = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.3, top_k=30, top_p=0.98, return_full_text=False)\n",
    "    outputs = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.2, top_k=20, top_p=0.95, return_full_text=False)\n",
    "    print(\"Time Taken = \", time.time()-t1, \" seconds\")\n",
    "    print(\"\\nSystem Output:\\n\")\n",
    "    print(outputs[0][\"generated_text\"])\n",
    "    print(\"==\"*50)\n",
    "    print(\"\\n\\n\")\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ec6635a4-cdbc-4dc3-b5c4-7f3b647e3011",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What are transformers?',\n",
       " 'How is this course graded?',\n",
       " 'Who is the professor for this course?',\n",
       " 'Compare text-dependent and text-independent speaker verification',\n",
       " 'What are the components of a Virtual Assistant?',\n",
       " 'How can we convert audio data into a learnable form?']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subjective_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7604cdf5-8e0e-4aaa-ad9c-d8cadcd76ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 150, but your input_length is only 62. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=31)\n",
      "Your max_length is set to 150, but your input_length is only 69. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=34)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Question:  What are transformers? \n",
      "\n",
      "Context Extracted: transformer block each of which is a multilayer network that map sequence of input vector x1xnto sequence of output vector z1znof the same length these block are made by combin ing simple linear layer feedforward network and selfattention layer . transformer selfattention allows a network to directly extract and use information from arbitrarily large context we ll start by describing how self attention work . et al introduced a diffusion transformer dit which facilitates use of the transformer architecture for diffusionbased image production also google released a transformercentric image generator called muse based on parallel decoding and masked generative transformer technology transformer played a lesscentral role with the transformer . xiong et al this is called preln transformer in 2023 . transformer predecessor of attention mechanism was added to gated recurrent .\n",
      "\n",
      " System output: \n",
      "\n",
      "**Option 1:** Which of the following is a valid pair of question types that are studied for QA assistants?\n",
      "a. Multiple choice and open ended\n",
      "b. Multiple choice and short answer\n",
      "c. Open ended and short answer\n",
      "**Answer: a. Multiple choice and open ended**\n",
      "\n",
      "**Option 2:** Which of the following is a valid pair of question types that are studied for QA assistants?\n",
      "a. Multiple choice and short answer\n",
      "b. Multiple choice and open ended\n",
      "c. Open ended and short answer\n",
      "**Answer: b. Multiple choice and short answer**\n",
      "Time Taken =  146.6908895969391  seconds\n",
      "\n",
      "\n",
      "\n",
      "======================================================================================================================================================\n",
      "\n",
      "\n",
      "Question:  How is this course graded? \n",
      "\n",
      "Context Extracted: \n",
      "\n",
      " System output: \n",
      "\n",
      "**Option 1**: Multiple Choice and Short Answer\n",
      "**Option 2**: True/False and Open-ended\n",
      "**Option 3**: Essay and Short Answer\n",
      "**Option 4**: Multiple Choice and Essay\n",
      "Time Taken =  52.109407901763916  seconds\n",
      "\n",
      "\n",
      "\n",
      "======================================================================================================================================================\n",
      "\n",
      "\n",
      "Question:  Who is the professor for this course? \n",
      "\n",
      "Context Extracted: \n",
      "\n",
      " System output: \n",
      "\n",
      "**Option 1:** Multiple Choice and Open-ended\n",
      "\n",
      "**Option selected:** Option 1\n",
      "\n",
      "**Explanation:** Both multiple choice and open-ended questions assess a user's ability to understand and respond to different types of prompts and questions.\n",
      "Time Taken =  59.25857615470886  seconds\n",
      "\n",
      "\n",
      "\n",
      "======================================================================================================================================================\n",
      "\n",
      "\n",
      "Question:  Compare text-dependent and text-independent speaker verification \n",
      "\n",
      "Context Extracted: speaker recognition is the task of identifying a speaker using their voice recognition is classified into two part speaker identification and speaker verification . speaker identification is the process of determining which voice in a group of known voice best match the speaker speaker verification is computationally le complex than speaker identification system because they require a comparison between only one or two model whereas speaker identification requires comparison of one model to n speaker modelsspeaker verification method are divided into textdependent and textindependent method the speaker verification system ha prior knowledge about the text to be spoken this browser is no longer supportedupgrade to microsoft edge to take advantage of the latest feature security update and technical supportspeaker recognition can help determine who is speaking in an audio clip the service can verify and identify speaker by their unique voice characteristic by using voice biometryyou can then crosscheck audio voice sample against this profile to see if it matches any profile in the group speaker identificationimportantmicrosoft limit access to speaker recognition you can apply for access through the azure ai service speaker recognition limited access review . user speech it then search the language model for the equivalent series of phonemes if a match is made it return the text of the corresponding word or phrase to the calling program architecture of an asr systemspeech recognition system can be classified on the basis of the constraint under which they are developed and which they consequently impose on their user these constraint include speaker dependence type of utterance size of the vocabulary linguistic constraint type of speech and environment of use we will describe each constraint a followsspeaker dependence speaker dependent channelsession variability includechannel mismatch between enrolment and verification speech signal such a using different microphone . difference in speaker voice such as ageing health speaking style and emotional statetransmission channel . voice over internet protocol voipmany frontend processing are often used to process the speech signal and to extract the feature . verification is measured in term of error the type of error and evaluation metric commonly used in speaker verification system are the followingfalse acceptance a false acceptance occurs when the speech segment from an imposter speaker is falsely accepted a target speaker by the verification systemsthe performance metric can be measured using the equal error rate eer and minimum decision cost function mdcf these measure represent different performance characteristic of system . the accuracy of the measurement is based on the number of trial evaluated in order to robustly compute the relevant statistic\n",
      "\n",
      " System output: \n",
      "\n",
      "**Option 1:** What is the task of speaker identification?\n",
      "a. Determining which voice in a group of known voices best match the speaker\n",
      "b. Determining which voice in a group of known voices best match the speaker\n",
      "c. Determining which voice in a group of known voices best match the speaker\n",
      "d. Determining which voice in a group of known voices best match the speaker\n",
      "\n",
      "\n",
      "**Option 2:** Which of the following is a speaker verification method?\n",
      "a. Text-dependent\n",
      "b. Text-independent\n",
      "c. Both text-dependent and text-independent\n",
      "d. None of the above\n",
      "\n",
      "\n",
      "**Answer:** A\n",
      "Time Taken =  178.10954928398132  seconds\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 150, but your input_length is only 121. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=60)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 150, but your input_length is only 100. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Question:  What are the components of a Virtual Assistant? \n",
      "\n",
      "Context Extracted: virtual assistant used to refer only to contract worker who perform administrative task from home but in the digital now the name evolved to define application and software that can do task and complete service for a user based on voice command and question you can think of them a your executive assistant or secretary in digital form virtual assistant also go by other similar term like ai assistant digital assistant and intelligent personal assistant . virtual assistant and their designer are controversial for spurring job insecurity and the ai they propose are still human in the way that they would be impossible without the microwork of million of human workersprivacy concern are raised by the fact that voice command are available to the provider of virtual assistant in unencrypted form and can thus be processed in an unauthorized or unexpected manner additionally to the linguistic content of recorded speech . google provides the action on google and dialogflow platform for developer to create action for google assistant apple provides sirikit microsoft a virtual assistant become more popular there are increasing legal risk involved 815 device and object virtual assistant may be integrated into many type of platform or like amazon alexa across several of them into device like smart speaker such a amazon echo google home and apple homepod in instant messaging application on both smartphones and via the web eg m virtual assistant on both facebook and facebook messenger apps . virtual assistant can provide information that s normally searchable in web browser news weather fact etc user can order them to set alarm get direction make shopping list and add event to the calendar these digital assistant can also play your favorite song from streaming service a well a check and control your smart home devicesthe main technology that power virtual assistant all fall under artificial intelligence voice and speech recognition . google assistant such a google assistant which includes google lens and bixby on the samsung galaxy series have the added capability of performing image processing to recognize object in imagesmany virtual assistant are accessible via multiple method offering versatility in how user can interact with them . virtual assistant use natural language processing nlp to match user text or voice input to executable command some continually learn using artificial intelligence technique including machine learning and ambient intelligence to activate a virtual assistant .\n",
      "\n",
      " System output: \n",
      "\n",
      "**Option 1:** Which of the following is a valid pair of question types that are studied for QA assistants?\n",
      "a. Multiple choice and open-ended\n",
      "b. True or false\n",
      "c. Short answer and essay\n",
      "d. Numerical and graphical\n",
      "\n",
      "\n",
      "**Option 2:** Which of the following is a valid pair of question types that are studied for QA assistants?\n",
      "a. Multiple choice and open-ended\n",
      "b. True or false\n",
      "c. Short answer and essay\n",
      "d. Numerical and graphical\n",
      "Time Taken =  140.44515824317932  seconds\n",
      "\n",
      "\n",
      "\n",
      "======================================================================================================================================================\n",
      "\n",
      "\n",
      "Question:  How can we convert audio data into a learnable form? \n",
      "\n",
      "Context Extracted: baidu silicon valley artificial intelligence lab deep voice is a texttospeech system developed using deep neural networksit ha five major building blocksthe graphemetophoneme model convert english character to phoneme the segmentation model identifies where each phoneme begin and end in an audio file the phoneme duration model predicts whether a phoneme is voiced the fundamental frequency model synthesizes audio . the author introduce a wavenetbased spectrogramtoaudio neural vocoder which is then used with taco this article will look at research and model architecture that have been written and developed to do just that using deep learning . the concatenative approach speech from a large database of audio voice is used this limit the scalability of this approachthe parametric approach us a recorded human voice and a function with a set of parameter that can be modified to change the voicethese two approach represent the old way of doing speech synthesis now let s look at the new way . the inverse fourier transform is applied to the output on the right to characterize f1 f2 and f3 detail laterin addition there is duality in the fourier transformation convolution and multiplication can be interchanged asso if a function is hard to model we may model it in another domain on the other hand if the manipulation is hard we can perform the corresponding duality transformation above to see whether it may be easier to solve conceptually we are just switching tool whenever it is easierto extract audio feature we\n",
      "\n",
      " System output: \n",
      "\n",
      "**Option 1:** Which of the following is a valid pair of question types that are studied for QA assistants?\n",
      "a. Multiple choice and open-ended\n",
      "b. True/false and multiple choice\n",
      "c. Short answer and essay\n",
      "d. Numerical and graphical\n",
      "\n",
      "\n",
      "**Option 2:** Which of the following is a valid pair of question types that are studied for QA assistants?\n",
      "a. Multiple choice and open-ended\n",
      "b. True/false and multiple choice\n",
      "c. Short answer and essay\n",
      "d. Numerical and graphical\n",
      "\n",
      "\n",
      "**Answer:** a. Multiple choice and open-ended\n",
      "\n",
      "**Explanation:** The passage is\n",
      "Time Taken =  169.94454956054688  seconds\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for Q in subjective_questions:\n",
    "    t1 = time.time()\n",
    "    q_emb = emb_model.encode(Q.lower())\n",
    "    retrieved_text = answer_question(Q.lower(), inverted_index, corpus, vectorizer, n=5)\n",
    "    score = [cosine(emb_model.encode(x), q_emb) for x in retrieved_text]\n",
    "    relevant_text = filter_context(retrieved_text, score, relevance_threshold, baseline=True)\n",
    "    print(\"==\"*75)\n",
    "            \n",
    "    #relevant_text = [element for sublist in relevant_text for element in sublist]\n",
    "    relevant_text = summarizer(relevant_text, max_length=150, min_length=30, do_sample=False)\n",
    "    relevant_text = \" \".join(d['summary_text'] for d in relevant_text)\n",
    "\n",
    "    txt = \"\"\n",
    "    \n",
    "        \n",
    "    input_text = f\"Context: {relevant_text} \\n [Question]: {extracted_info['Question']} [Options]: {txt}\"\n",
    "    \n",
    "    print(\"\\n\\nQuestion: \", Q, \"\\n\")\n",
    "    print(f\"Context Extracted: {relevant_text}\")\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": mcq_mission,\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"ok\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": input_text\n",
    "        },\n",
    "        \n",
    "    ]\n",
    "    \n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    outputs = pipe(prompt, max_new_tokens=128, do_sample=True, temperature=0.3, top_k=30, top_p=0.98, return_full_text=False)\n",
    "    print(\"\\n System output: \\n\")\n",
    "    print(outputs[0][\"generated_text\"])\n",
    "    print(\"Time Taken = \", time.time()-t1, \" seconds\")\n",
    "    print(\"\\n\\n\")\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ede390-d7a2-4353-861a-04062a928b65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdadf1ba-41a8-4eec-8f6d-2f6ce77ebf6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87cfa04a-21cd-4d8a-a58e-ecdeb46b1afb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
